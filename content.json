{"meta":{"title":"YMLiang","subtitle":"Never forget why you started","description":"YMLiang's blog","author":"YMLiang","url":"http://yoursite.com","root":"/"},"pages":[{"title":"categories","date":"2019-02-18T07:06:03.000Z","updated":"2019-02-18T07:07:49.535Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"非凡","date":"2019-05-30T07:02:47.805Z","updated":"2019-05-30T07:02:47.805Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"96base BeiJinLove Lv mn"},{"title":"friends","date":"2019-02-18T07:12:33.000Z","updated":"2019-02-18T07:12:49.434Z","comments":true,"path":"friends/index.html","permalink":"http://yoursite.com/friends/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-02-18T07:10:12.000Z","updated":"2019-05-30T06:56:45.059Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"beego框架搭建踩坑","slug":"beego框架搭建踩坑","date":"2019-05-30T05:39:38.000Z","updated":"2019-05-30T05:39:38.203Z","comments":true,"path":"2019/05/30/beego框架搭建踩坑/","link":"","permalink":"http://yoursite.com/2019/05/30/beego框架搭建踩坑/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"spark知识梳理","slug":"spark知识梳理","date":"2019-05-23T10:19:30.000Z","updated":"2019-05-30T07:17:37.938Z","comments":true,"path":"2019/05/23/spark知识梳理/","link":"","permalink":"http://yoursite.com/2019/05/23/spark知识梳理/","excerpt":"Spark定义:Spark编程始于数据集，而数据集往往存放在分布式持久化存储之上，比如Hadoop分布式文件系统HDFS 编写Spark 程序通常包括一系列相关步骤。 • 在输入数据集上定义一组转换。 • 调用 action，用以将转换后的数据集保存到持久存储上，或者把结果返回到驱动程序的 本地内存。 • 运行本地计算，本地计算处理分布式计算的结果。本地计算有助于你确定下一步的转换 和 action。","text":"Spark定义:Spark编程始于数据集，而数据集往往存放在分布式持久化存储之上，比如Hadoop分布式文件系统HDFS 编写Spark 程序通常包括一系列相关步骤。 • 在输入数据集上定义一组转换。 • 调用 action，用以将转换后的数据集保存到持久存储上，或者把结果返回到驱动程序的 本地内存。 • 运行本地计算，本地计算处理分布式计算的结果。本地计算有助于你确定下一步的转换 和 action。 要想理解 Spark，就必须理解 Spark 框架提供的两种抽象：存储和执行。Spark 优美地搭配 这两类抽象，可以将数据处理管道中的任何中间步骤缓在内存里以备后用 常见的RDD转化操作 transform算子: action算子: case classcase class 是不可变类的一种简单类型它非常好用，内置了所有 Java 类的基本方法，比如toString、equals和 hashCode。我们来试试为记录关联数据定义一个 case class： 数据分析1234样例数据来自加州大学欧文分校机器学习资料库（UC Irvine Machine Learning Repository）。这里要分析的数据集来源于一项纪录关联研究，是德国一家医院在2010年完成的。这个数据及包含数百万对病人记录，每队记录都根据不同标准来匹配。比如病人姓名、地址、生日。每个匹配字段都被赋予一个数值评分，范围为0.0 到 1.0，分值根据字符串相似度得出。然后这些数据交给人工处理，标记出哪些代表同一个人哪些代表不同的人。为了保护病人隐私，创建的数据集的每个字段原始值被删除。病人的ID、字段匹配分数、匹配对标识（包括匹配的和不匹配的）等信息是公开的，可用于纪录关联研究。 数据获取 $ mkdir linkage $ cd linkage/ $ wget https://archive.ics.uci.edu/ml/machine-learning-databases/00210/donation.zip (http://bit.ly/1Aoywaq) $ unzip donation.zip $ unzip &apos;block_*.zip&apos; 放入HDFS： $ hadoop fs -mkdir /linkage $ hadoop fs -put block_*.csv /linkage 启动spark-shell 这里本地启动 spark-shell Setting default log level to &quot;WARN&quot;. To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). 19/05/24 07:15:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Spark context Web UI available at http://192.111.110.111:4040 Spark context available as &apos;sc&apos; (master = spark://master:7077, app id = app-20190524071537-0001). Spark session available as &apos;spark&apos;. Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ &apos;_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 2.2.0 /_/ Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_144) Type in expressions to have them evaluated. Type :help for more information. scala&gt; 如果出现以上说明启动成功 读数据 scala&gt;val rawblocks = sc.textFile(&quot;hdfs:///linkage/*.csv&quot;) scala&gt;val head = rawblocks.take(10) scala&gt;head.length res2: Int = 10 遍历 scala&gt;head.foreach(println) &quot;id_1&quot;,&quot;id_2&quot;,&quot;cmp_fname_c1&quot;,&quot;cmp_fname_c2&quot;,&quot;cmp_lname_c1&quot;,&quot;cmp_lname_c2&quot;,&quot;cmp_sex&quot;,&quot;cmp_bd&quot;,&quot;cmp_bm&quot;,&quot;cmp_by&quot;,&quot;cmp_plz&quot;,&quot;is_match&quot; 37291,53113,0.833333333333333,?,1,?,1,1,1,1,0,TRUE 39086,47614,1,?,1,?,1,1,1,1,1,TRUE 70031,70237,1,?,1,?,1,1,1,1,1,TRUE 84795,97439,1,?,1,?,1,1,1,1,1,TRUE 36950,42116,1,?,1,1,1,1,1,1,1,TRUE 42413,48491,1,?,1,?,1,1,1,1,1,TRUE 25965,64753,1,?,1,?,1,1,1,1,1,TRUE 49451,90407,1,?,1,?,1,1,1,1,0,TRUE 39932,40902,1,?,1,?,1,1,1,1,1,TRUE 定义函数过滤第一行的表头数据：(“id_1”,”id_2”,”cmp_fname_c1”….) scala&gt; def isHeader(line:String):Boolean={ line.contains(&quot;id_1&quot;) || line.contains(&quot;cmp&quot;) } scala&gt; head.filter(isHeader).foreach(println) 查包含&quot;id_1&quot;的行 scala&gt; head.filterNot(isHeader).foreach(println) 查不包含&quot;id_1&quot;的行 scala&gt; head.filter(x=&gt; isHeader(x)).foreach(println) 查包含&quot;id_1&quot;的行 scala&gt; head.filter(x=&gt; !isHeader(x)).foreach(println) 查不包含&quot;id_1&quot;的行 scala&gt; head.filter(isHeader(_)).foreach(println) 查包含&quot;id_1&quot;的行 scala&gt; head.filter(!isHeader(_)).foreach(println) 查不包含&quot;id_1&quot;的行 过滤的方法有三种，再上方已经列举出来了，可以根据喜好和编码习惯选择 将去掉标题头 即含有 id_1的第一行去掉后变为新的 RDD scala&gt; val noheader = rawblocks.filter(x =&gt; !isHeader(x)) noheader: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at filter at &lt;console&gt;:28 进行数据结构化 说到数据结构化，首先java的思想就是封装，scala中是元组和case class ，其中 case class类似于封装 tuple 元组 -- scala 为了测试方便，先取一行数据 val line = head(5) 1. 首先定义一个函数，里面处理每一行数据 def parse(line:String)={ val pieces = line.split(&apos;,&apos;) val id1 = pieces(0).toInt val id2 = pieces(1).toInt val scores = pieces.slice(2,11).map(toDouble) val matched = pieces(11).toBoolean (id1,id2,scores,matched) } val tup = parse(line) 2. 可能你没有注意有问号的情况，那么为了避免出现 ? 导致程序报错，可以定义 toDouble函数 def toDouble(s:String)={ if(&quot;?&quot;.equals(s)) Double.NaN else s.toDouble } -------------------------------------------------------------------------- slice函数截取集合 slice函数需要两个参数，第一个参数表示从该下标开始截取，第二个参数表示截取到该下标（不包含）。 object TestCollection { val list =List(1,4,6,4,1) def main(args: Array[String]): Unit = { print(list.slice(0,3)) } } -------------------------------------------------------------------------- 3. 之后再执行 val tup = parse(line) 得到的tup就是一个元组 元组的操作: tup._1 取第一个元素 tup.productElement(0) 或者用 productElement 方法，它是从 0 开始计数的。 tup.productArity 也可以用 productArity 方法得到元组的大小 case class case class 是不可变类的一种简单类型，它非常好用，内置了所有 Java 类的基本方法，比如 toString、equals 和 hashCode。我们来试试为记录关联数据定义一个 case class： case class MatchData(id1: Int, id2: Int,scores: Array[Double], matched: Boolean) 现在修改 parse 方法以返回 MatchData 实例，这个实例是 case class 而不再是元组： def parse(line: String) = { val pieces = line.split(&apos;,&apos;) val id1 = pieces(0).toInt val id2 = pieces(1).toInt val scores = pieces.slice(2, 11).map(toDouble) val matched = pieces(11).toBoolean MatchData(id1, id2, scores, matched) } val md = parse(line) 这里要注意两点： 一，创建 case class 时没必要在 MatchData 前写上关键字 new 二，MatchData 类有个内置的 toString 方法实现，除了scores 数组字段外，这个方法在其他字段上的表现都还不错。 现在通过名字来访问 MatchData 的字段： md.matched md.id1 是不是有种 对象.成员变量 的意思 有些地方还是很像java的，比如java8的lambda表达式和scala非常相似 之后将其作用于head数据集上 val mds = head.filter(x=&gt; !isHeader(x)).map(x=&gt; parse(x)) 之后再作用于整个数据集上 noheader val parsed = noheader.map(line =&gt; parse(line)) 记住：和我们本地生成的 mds 数组不同，parse 函数并没有实际应用到集群数据上。当在 parsed 这个 RDD 上执行某个需要输出的调用时，就会用 parse 函数把 noheader RDD 的每个 String 转换成 MatchData 类的实例。如果在 parsed RDD 上执行另一个调用以 产生不同输出，parse函数会在输入数据上再执行一遍。这没有充分利用集群资源。数据一 旦解析好，我们想以解析格式把数据存到集群上，这样就不需要每次遇到新问题时都重新解 析。Spark支持这种使用场景，通过在实例上调用cache 方法，可以指示在内存里缓存某个 RDD。现在用 parsed 这个 RDD 实验一下： parsed.cache() Spark的缓存： 虽然默认情况下 RDD 的内容是临时的，但 Spark 提供了在 RDD 中持久化数据的机制。第 一次调用动作并计算出 RDD 内容后，RDD 的内容可以存储在集群的内存或磁盘上。这样下一 次需要调用依赖该 RDD 的动作时，就不需要从依赖关系中重新计算 RDD，数据可以从缓存分 区中直接返回： cached.cache() cached.count() cached.take(10) 在上述代码中，cache 方法调用指示在下次计算 RDD 后，要把 RDD 存储起来。调用 count 会导致第一次计算 RDD。采取（take）这个动作返回一个本地的 Array，包含 RDD 的前 10 个元素。但调用 take 时，访问的是 cached 已经缓存好的元素，而不是 从 cached 的依赖关系中重新计算出来的。 Spark 为持久化 RDD 定义了几种不同的机制，用不同的 StorageLevel 值表示。rdd. cache() 是 rdd.persist(StorageLevel.MEMORY) 的简写，它将 RDD 存储为未序列化 的 Java 对象。当 Spark 估计内存不够存放一个分区时，它干脆就不在内存中存放该分 区，这样在下次需要时就必须重新计算。在对象需要频繁访问或低延访问时适合使用 StorageLevel.MEMORY，因为它可以避免序列化的开销。相比其他选项，StorageLevel. MEMORY 的问题是要占用更大的内存空间。另外，大量小对象会对 Java 的垃圾回收造成 压力，会导致程序停顿和常见的速度缓慢问题。 Spark 也提供了 MEMORY_SER 的存储级别，用于在内存中分配大字节缓冲区以存储 RDD 序列化内容。如果使用得当（稍后会详细介绍），序列化数据占用的空间比未经序列化 的数据占用的空间往往要少两到五倍。 Spark 也可以用磁盘来缓存 RDD。 存储级别 MEMORY_AND_DISK 和 MEMORY_AND_DISK_SER 分别类似于 MEMORY 和 MEMORY_SER。对于 MEMORY 和 MEMORY_SER，如果一个分区在内存 里放不下，整个分区都不会放在内存。对于 MEMORY_AND_DISK 和 MEMORY_AND_DISK_SER， 如果分区在内存里放不下，Spark 会将其溢写到磁盘上。 什么时候该缓存数据是门艺术，这通常需要对空间和速度进行权衡，垃圾回收开销的 问题也会时不时让情况更复杂。一般情况下，如果多个动作需要用到某个 RDD，而它 的计算代价又很高，那么就应该把这个 RDD 缓存起来。 聚合 我们用 groupBy 方法来创建一个 Scala Map[Boolean, Array[MatchData]]，其中键值基于MatchData 类的字段 matched： val grouped = mds.groupBy(md =&gt; md.matched) grouped: scala.collection.immutable.Map[Boolean,Array[MatchData]] = Map(true -&gt; Array(MatchData(37291,53113,[D@14587d48,true), MatchData(39086,47614,[D@556e8e2e,true), MatchData(70031,70237,[D@4d94e219,true), MatchData(84795,97439,[D@25937e5b,true), MatchData(36950,42116,[D@7276f59c,true), MatchData(42413,48491,[D@53481a02,true), MatchData(25965,64753,[D@3926be58,true), MatchData(49451,90407,[D@7f96e81,true), MatchData(39932,40902,[D@42e6a1f7,true))) 利用mapValues进行记数 grouped.mapValues(x=&gt;x.size).foreach(println) (true,9) 创建直方图 先来试试创建一个简单的直方图，用它来算一下 parsed 中的 MatchData 记录有多少 matched 字段值为 true 或 false。幸运的是 RDD[T] 类已经定义了一个名为 countByValue 的动作，该 动作对于计数类运算效率非常高，它向客户端返回 Map[T,Long] 类型的结果。对 MatchData 记录中的 matched 字段映射调用 countByValue 会执行一个 Spark 作业，并向客户端返回结果： scala&gt; val matchCounts = parsed.map(md =&gt; md.matched).countByValue() countByValue:统计一个RDD中各个value的出现次数。返回一个map，map的key是元素的值，value是出现的次数。 count: 统计RDD中元素的个数。 countByKey: 与count类似，但是是以key为单位进行统计。 注意：此函数返回的是一个map，不是int。 Scala 的Map 类没有提供根据内容的键或值排序的方法，但是我们可以将 Map 转换成Scala 的Seq 类型，而Seq 支持排序。 Scala 的 Seq 类和 Java 的 List 类接口类似，都是可迭代集合，即具有确定的长度并且可以根据下标来查找值。 scala&gt; val matchCountsSeq = matchCounts.toSeq matchCountsSeq: Seq[(Boolean, Long)] = ArrayBuffer((true,20931), (false,5728201)) ------------------------------------------------------------------------------------------------ Scala 集合 Scala 集合类库很庞大，包括 list、set、map 和 array。利用 toList、toSet 和 toArray 方法，各种集合类型可以方便地相互转换。 ------------------------------------------------------------------------------------------------ sortBy 排序 matchCountsSeq.sortBy(_._1).foreach(println) matchCountsSeq.sortBy(_._2).foreach(println) matchCountsSeq.sortBy(_._2).reverse.foreach(println) _._1 指按第一列指标排序 reverse 反转排序结果","categories":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/categories/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"redis取经之路","slug":"redis取经之路","date":"2019-05-21T06:44:34.000Z","updated":"2019-05-29T07:00:25.431Z","comments":true,"path":"2019/05/21/redis取经之路/","link":"","permalink":"http://yoursite.com/2019/05/21/redis取经之路/","excerpt":"","text":"###redis基本数据结构 Redis使用的是自己构建的简单动态字符串(SDS)[simple dynamic string,SDS]的抽象类型，并将SDS用做Rdis的默认字符串表示12redis&gt; SET msg &quot;hello&quot;Ok redis的Key-Value的存储方式 key是一个字符串对象，对象的底层实现是一个保存着字符串&quot;msg&quot;的SDS value也是一个字符串对象，对象的底层实现是一个保存着字符串&quot;hello&quot;的SDS SDS除了用来保存数据库中的字符串值之外，还被用作缓冲区(Buffer):AOF模块中的AOF缓冲区 基本数据类型 - 字符串命令:GET SET DEL - 列表:一个列表结构可以有序地存储多个字符串 LPUSH RPUSH 表示元素推入列表的左端和右端 LPOP RPOP分别从列表的左端和右端弹出元素，被弹出的元素不再属于列表 LINDEX用于获取列表在给定位置的上一个元素 LRANGE用于获取列表在给定范围上的所有元素 - 集合:列表可以存储多个相同的字符串，集合则通过散列表来保证自己存储的每个字符串都是各不相同的(这些散列表只有键，但没有与键相关联的值) redis的集合使用的是无序方式存储元素 SADD:将元素添加到集合 redis&gt; sadd fengjr &quot;hehehehe&quot; (integer) 1 SREM:从集合移除元素，命令会返回被移除元素的数量 SMEMBERS:获取集合包含的所有元素将得到一个由元素组成的序列 SISMEMBER:快速检查一个元素是否已经存在于集合中 - 散列:redis的散列可以存储多个键值对之间的映射。和字符串一样，散列存储的值既可以是字符串，又可以是数字值，并且用户同样可以对散列存储的数字值执行自增操作或者自减操作 HSET:在散列里面关联起给定的键值对 hset hash-key sub-key1 value1 hset hash-key sub-key2 value2 栗子: redis&gt; HSET myhash field1 123 redis&gt; HSET myhash name liangym 返回值为1表示给定的键不存在于散列里面，添加成功 返回值为0表示给定的键存在于散列里面，添加失败 HDEL:如果给定键存在于散列里面，删除这个键 栗子: redis&gt; HDEL myhash name (integer) 1 删除成功 HGET:从散列里面获取某个键的值 HGETALL:获取散列包含的所有键值对 栗子: redis&gt; HGETALL myhash 1) &quot;filed1&quot; 2) &quot;foo&quot; 3) &quot;field1&quot; 4) &quot;123&quot; HINCRBY:对散列存储的值执行自增操作 HINCRBY key field icrement 栗子: redis&gt;HINCRBY myhash field1 123 (integer) 246 - 有序集合:和散列一样都用于存储键值对：有序集合的键被称为成员(member)，每个成员都是各不相同的；而有序集合的值则被称为分值(score)，分值必须为浮点数 ZADD:将一个带有给定分值的成员添加到有序集合里面 栗子: redis&gt;zadd zset-key 728 member ZRANGE:根据元素在有序排列中所处的位置，从有序集合里面获取多个元素 ZRANGEBYSCORE:获取有序集合在给定分值范围内的所有元素 ZREM:如果给定成员存在于有序集合，那么移除这个成员 ZINCRBY:给有序集合成员的分值执行自增操作 栗子: ZINCRBY key increment member redis&gt;ZINCRBY zset-key 123 member1 ZSCORE:检查记录的有序集合的值 栗子: ZSCORE key member redis&gt;ZSCORE zset-key member1 &quot;851&quot; redis的特性数据结构 内存存储(这使得Redis的速度非常快) 远程(这使得Redis可以与多个客户端和服务器进行连接) 持久化(这可以使得服务器可以再重启之后仍然保持重启之前的数据) 可扩展(通过主从复制和分片) 通过将传统数据库的一部分数据处理任务以及存储任务转交给redis来完成，可以提升网页的载入速度，并降低资源的占用量 场景一：登录和cookie缓存将登录信息存储在cookie中 可以用签名(signed) 和令牌(token) 使用redis来记录用户信息，将每天要对数据库执行的写入操作减少了很多 场景二：购物车定义： 散列：每个用户的购物车 key：商品ID value：商品订购数量 将会话和购物车都存储到Redis里面，这样可以减少请求的体积，还可以使得我们根据用户浏览过的上铺，用户放入购物车的商品，以及用户最终购买的商品进行统计计算。 实现：在查看过这件商品的用户当中，有X%的用户最终购买了这件商品， 购买了这件商品的用户也购买了某某某其他商品 等功能 网页缓存页面内容不需要动态生成，减少网站在动态生成内容上面所花的时间，可以降低网站处理相同负载所需的服务器数量，并让网站的速度变得更快(增强用户体验) 中间层的作用：对于一个不能被缓存的请求，函数将直接生成并返回页面，而对于可以被缓存的请求，函数首先会尝试从缓存里取出并返回被缓存的页面，如果缓存页面不存在，那么函数会生成页面并将其缓存在Redis中5分钟，最后将页面返回给函数调用者 数据行缓存场景：促销活动，会推出一些特价商品供用户抢购 疑虑：网站如果对整个促销页面进行缓存，可能会导致用户看到错误的特价商品的数量，但如果每次都从数据库里面取出特价商品的剩余数量的话又会给数据库带来巨大的压力，导致我们需要花费额外的成本来扩展数据库。 解决：促销活动必定会带来一些负载，所以必须对数据进行缓存，编写一个持续运行的守护进程，让这个函数将指定的数据行缓存到redis里面，并不定期地对这些缓存进行更新。 设置两个有序集合，分别为：调度有序集合，延时有序集合，调度顾名思义调度，延时则是设置了指定数据航的缓存需要每隔多少秒更新一次 如果数据行记录的是特价促销商品的剩余数量，并且参与促销活动的用户非常多的话，那么我们最好每隔几秒更新一次数据行缓存；另一方面，如果数据并不经常改变，或者商品缺货是可以接受的，那么我们可以将更新缓存的时间变长 网页分析redis事务","categories":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/categories/redis/"}],"tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"Git配置SSH密钥","slug":"Git配置SSH密钥","date":"2019-05-16T01:48:08.000Z","updated":"2019-05-16T03:29:05.312Z","comments":true,"path":"2019/05/16/Git配置SSH密钥/","link":"","permalink":"http://yoursite.com/2019/05/16/Git配置SSH密钥/","excerpt":"","text":"配置用户名和邮箱在随便一个文件夹空白处右击 点击 Git Bash Here 初次安装git配置用户名和邮箱 $ git config --global user.name &quot;yiming.liang&quot; $ git config --global user.email &quot;18135479521@163.com&quot; 注意：（引号内请输入你自己设置的名字，和你自己的邮箱）此用户名和邮箱是git提交代码时用来显示你身份和联系方式的，并不是github用户名和邮箱 git使用ssh密钥 -------------------------------------------------------------------------------- git使用https协议，每次pull, push都会提示要输入密码，使用git协议，然后使用ssh密钥，这样免去每次都输密码的麻烦 初次使用git的用户要使用git协议大概需要三个步骤： 一、生成密钥对 二、设置远程仓库（本文以github为例）上的公钥 三、把git的 remote url 修改为git协议（以上两个步骤初次设置过以后，以后使用都不需要再次设置，此步骤视以后项目的remote url而定，如果以后其他项目的协议为https则需要此步骤） 一、生成密钥对 -------------------------------------------------------------------------------- 大多数 Git 服务器都会选择使用 SSH 公钥来进行授权。系统中的每个用户都必须提供一个公钥用于授权，没有的话就要生成一个。生成公钥的过程在所有操作系统上都差不多。首先你要确认一下本机是否已经有一个公钥。 SSH 公钥默认储存在账户的主目录下的 ~/.ssh 目录。进去看看： $ cd ~/.ssh $ ls authorized_keys2 id_dsa known_hosts config id_dsa.pub 看一下有没有id_rsa和id_rsa.pub(或者是id_dsa和id_dsa.pub之类成对的文件)，有 .pub 后缀的文件就是公钥，另一个文件则是密钥。 假如没有这些文件，甚至连 .ssh 目录都没有，可以用 ssh-keygen 来创建。该程序在 Linux/Mac 系统上由 SSH 包提供，而在 Windows 上则包含在 MSysGit 包里： $ ssh-keygen -t rsa -C &quot;your_email@youremail.com&quot; Creates a new ssh key using the provided email # Generating public/private rsa key pair. Enter file in which to save the key (/home/you/.ssh/id_rsa): 直接按Enter就行。然后，会提示你输入密码，如下(建议输一个，安全一点，当然不输也行，应该不会有人闲的无聊冒充你去修改你的代码)： Enter same passphrase again: [Type passphrase again] 完了之后，大概是这样： Your public key has been saved in /home/you/.ssh/id_rsa.pub. The key fingerprint is: # 01:0f:f4:3b:ca:85:d6:17:a1:7d:f0:68:9d:f0:a2:db your_email@youremail.com 到此为止，你本地的密钥对就生成了。 二、添加公钥到你的远程仓库（github） 1、查看你生成的公钥： $ cat ~/.ssh/id_rsa.pub ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC0X6L1zLL4VHuvGb8aJH3ippTozmReSUzgntvk434aJ/v7kOdJ/MTyBlWXFCR+HAo3FXRitBqxiX1nKhXpHAZsMciLq8vR3c8E7CjZN733f5AL8uEYJA+YZevY5UCvEg+umT7PHghKYaJwaCxV7sjYP7Z6V79OMCEAGDNXC26IBMdMgOluQjp6o6j2KAdtRBdCDS/QIU5THQDxJ9lBXjk1fiq9tITo/aXBvjZeD+gH/Apkh/0GbO8VQLiYYmNfqqAHHeXdltORn8N7C9lOa/UW3KM7QdXo6J0GFlBVQeTE/IGqhMS5PMln3 admin@admin-PC 2、登陆你的github帐户。点击你的头像，然后 Settings -&gt; 左栏点击 SSH and GPG keys -&gt; 点击 New SSH key 3、然后你复制上面的公钥内容，粘贴进“Key”文本域内。 title域，自己随便起个名字。 4、点击 Add key。 完成以后，验证下这个key是不是正常工作： $ ssh -T git@github.com Attempts to ssh to github 如果，看到： Hi xxx! You&apos;ve successfully authenticated, but GitHub does not # provide shell access. 恭喜你，你的设置已经成功了。 三、修改git的remote url 使用命令 git remote -v 查看你当前的 remote url $ git remote -v origin https://github.com/someaccount/someproject.git (fetch) origin https://github.com/someaccount/someproject.git (push) 如果是以上的结果那么说明此项目是使用https协议进行访问的（如果地址是git开头则表示是git协议） 你可以登陆你的github，就像本文开头的图例，你在上面可以看到你的ssh协议相应的url，类似： 复制此ssh链接，然后使用命令 git remote set-url 来调整你的url。 git remote set-url origin git@github.com:someaccount/someproject.git 然后你可以再用命令 git remote -v 查看一下，url是否已经变成了ssh地址。 然后你就可以愉快的使用git fetch, git pull , git push，再也不用输入烦人的密码了 参考链接：http://www.tuicool.com/articles/BzUrAvF","categories":[{"name":"git","slug":"git","permalink":"http://yoursite.com/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"http://yoursite.com/tags/git/"}]},{"title":"leetCode 155 最小栈","slug":"leetCode-155-最小栈","date":"2019-05-06T10:04:32.000Z","updated":"2019-05-06T10:17:15.987Z","comments":true,"path":"2019/05/06/leetCode-155-最小栈/","link":"","permalink":"http://yoursite.com/2019/05/06/leetCode-155-最小栈/","excerpt":"设计一个支持 push，pop，top 操作，并能在常数时间内检索到最小元素的栈。 push(x) -- 将元素 x 推入栈中。 pop() -- 删除栈顶的元素。 top() -- 获取栈顶元素。 getMin() -- 检索栈中的最小元素。","text":"设计一个支持 push，pop，top 操作，并能在常数时间内检索到最小元素的栈。 push(x) -- 将元素 x 推入栈中。 pop() -- 删除栈顶的元素。 top() -- 获取栈顶元素。 getMin() -- 检索栈中的最小元素。 示例: MinStack minStack = new MinStack(); minStack.push(-2); minStack.push(0); minStack.push(-3); minStack.getMin(); --&gt; 返回 -3. minStack.pop(); minStack.top(); --&gt; 返回 0. minStack.getMin(); --&gt; 返回 -2. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class MinStack &#123; //声明栈 private Stack&lt;Integer&gt; stack; //声明初始最小值为Integer.MAX_VALUE保证第一个push的值必定毕它小则可以设定第一个push的值为最小值 private int min = Integer.MAX_VALUE; /** initialize your data structure here. */ public MinStack() &#123; //新建栈 stack = new Stack&lt;&gt;(); &#125; public void push(int x) &#123; //判断如果push的值小于等于min值则将之前的最小值入栈，min设置为当前最小值，之后将当前最小值入栈 // &gt;= 中的 等于是为了避免出现相同值的情况 比如 0,1,0 if(min&gt;=x)&#123; stack.push(min); min = x; &#125; stack.push(x); &#125; public void pop() &#123; //判断pop的值是否和当前最小值相等，如果相等则将上一次push的值设为最小值(因为pop后栈顶元素为上一次入栈的最小值)，整体思路就是每次遇到最小值时将上一次的最小值push，之后push当前最小值，pop的时候也是同样，如果pop的值等于最小值则将最小值设置为上一次push入的值，注意pop的时候，只要执行了pop()方法，栈顶元素就出栈了，则当前最小值出栈，此时栈顶元素为上一次push的最小值，所以可以直接设置为最小值(即 min = stack.pop()) if(stack.pop() == min)&#123; min = stack.pop(); &#125; &#125; public int top() &#123; //打印栈顶元素 return stack.peek(); &#125; public int getMin() &#123; //直接返回设定的最小值 return min; &#125;&#125;/*** 忽略这段* Your MinStack object will be instantiated and called as such:* MinStack obj = new MinStack();* obj.push(x);* obj.pop();* int param_3 = obj.top();* int param_4 = obj.getMin();*/","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://yoursite.com/categories/leetcode/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://yoursite.com/tags/leetcode/"}]},{"title":"linux高级命令","slug":"linux高级命令","date":"2019-05-06T06:48:25.000Z","updated":"2019-05-06T06:56:06.294Z","comments":true,"path":"2019/05/06/linux高级命令/","link":"","permalink":"http://yoursite.com/2019/05/06/linux高级命令/","excerpt":"","text":"awkawk是行处理器: 相比较屏幕处理的优点，在处理庞大文件时不会出现内存溢出或是处理缓慢的问题，通常用来格式化文本信息 awk处理过程: 依次对每一行进行处理，然后输出 awk命令形式: awk [-F|-f|-v] ‘BEGIN{} //{command1; command2} END{}’ file [-F|-f|-v] 大参数，-F指定分隔符，-f调用脚本，-v定义变量 var=value &apos; &apos; 引用代码块 BEGIN 初始化代码块，在对每一行进行处理之前，初始化代码，主要是引用全局变量，设置FS分隔符 // 匹配代码块，可以是字符串或正则表达式 {} 命令代码块，包含一条或多条命令 ； 多条命令使用分号分隔 END 结尾代码块，在对每一行进行处理之后再执行的代码块，主要是进行最终计算或输出结尾摘要信息 特殊要点: $0 表示整个当前行 $1 每行第一个字段 NF 字段数量变量 NR 每行的记录号，多文件记录递增 FNR 与NR类似，不过多文件记录不递增，每个文件都从1开始 \\t 制表符 \\n 换行符 FS BEGIN时定义分隔符 RS 输入的记录分隔符， 默认为换行符(即文本是按一行一行输入) ~ 匹配，与==相比不是精确比较 !~ 不匹配，不精确比较 == 等于，必须全部相等，精确比较 != 不等于，精确比较 &amp;&amp; 逻辑与 || 逻辑或 + 匹配时表示1个或1个以上 /[0-9][0-9]+/ 两个或两个以上数字 /[0-9][0-9]*/ 一个或一个以上数字 FILENAME 文件名 OFS 输出字段分隔符， 默认也是空格，可以改为制表符等 ORS 输出的记录分隔符，默认为换行符,即处理结果也是一行一行输出到屏幕 -F&apos;[:#/]&apos; 定义三个分隔符 print &amp; $0 print 是awk打印指定内容的主要命令 awk &apos;{print}&apos; /etc/passwd == awk &apos;{print $0}&apos; /etc/passwd awk &apos;{print &quot; &quot;}&apos; /etc/passwd //不输出passwd的内容，而是输出相同个数的空行，进一步解释了awk是一行一行处理文本 awk &apos;{print &quot;a&quot;}&apos; /etc/passwd //输出相同个数的a行，一行只有一个a字母 awk -F&quot;:&quot; &apos;{print $1}&apos; /etc/passwd awk -F: &apos;{print $1; print $2}&apos; /etc/passwd //将每一行的前二个字段，分行输出，进一步理解一行一行处理文本 awk -F: &apos;{print $1,$3,$6}&apos; OFS=&quot;\\t&quot; /etc/passwd //输出字段1,3,6，以制表符作为分隔符 -f指定脚本文件 awk -f script.awk file BEGIN{ FS=&quot;:&quot; } {print $1} //效果与awk -F&quot;:&quot; &apos;{print $1}&apos;相同,只是分隔符使用FS在代码自身中指定 awk &apos;BEGIN{X=0} /^$/{ X+=1 } END{print &quot;I find&quot;,X,&quot;blank lines.&quot;}&apos; test I find 4 blank lines. ls -l|awk &apos;BEGIN{sum=0} !/^d/{sum+=$5} END{print &quot;total size is&quot;,sum}&apos; //计算文件大小 total size is 17487 -F指定分隔符 $1 指指定分隔符后，第一个字段，$3第三个字段， \\t是制表符 一个或多个连续的空格或制表符看做一个定界符，即多个空格看做一个空格 awk -F&quot;:&quot; &apos;{print $1}&apos; /etc/passwd awk -F&quot;:&quot; &apos;{print $1 $3}&apos; /etc/passwd //$1与$3相连输出，不分隔 awk -F&quot;:&quot; &apos;{print $1,$3}&apos; /etc/passwd //多了一个逗号，$1与$3使用空格分隔 awk -F&quot;:&quot; &apos;{print $1 &quot; &quot; $3}&apos; /etc/passwd //$1与$3之间手动添加空格分隔 awk -F&quot;:&quot; &apos;{print &quot;Username:&quot; $1 &quot;\\t\\t Uid:&quot; $3 }&apos; /etc/passwd //自定义输出 awk -F: &apos;{print NF}&apos; /etc/passwd //显示每行有多少字段 awk -F: &apos;{print $NF}&apos; /etc/passwd //将每行第NF个字段的值打印出来 awk -F: &apos;NF==4 {print }&apos; /etc/passwd //显示只有4个字段的行 awk -F: &apos;NF&gt;2{print $0}&apos; /etc/passwd //显示每行字段数量大于2的行 awk &apos;{print NR,$0}&apos; /etc/passwd //输出每行的行号 awk -F: &apos;{print NR,NF,$NF,&quot;\\t&quot;,$0}&apos; /etc/passwd //依次打印行号，字段数，最后字段值，制表符，每行内容 awk -F: &apos;NR==5{print}&apos; /etc/passwd //显示第5行 awk -F: &apos;NR==5 || NR==6{print}&apos; /etc/passwd //显示第5行和第6行 route -n|awk &apos;NR!=1{print}&apos; //不显示第一行 //匹配代码块 //纯字符匹配 !//纯字符不匹配 ~//字段值匹配 !~//字段值不匹配 ~/a1|a2/字段值匹配a1或a2 awk &apos;/mysql/&apos; /etc/passwd awk &apos;/mysql/{print }&apos; /etc/passwd awk &apos;/mysql/{print $0}&apos; /etc/passwd //三条指令结果一样 awk &apos;!/mysql/{print $0}&apos; /etc/passwd //输出不匹配mysql的行 awk &apos;/mysql|mail/{print}&apos; /etc/passwd awk &apos;!/mysql|mail/{print}&apos; /etc/passwd awk -F: &apos;/mail/,/mysql/{print}&apos; /etc/passwd //区间匹配 awk &apos;/[2][7][7]*/{print $0}&apos; /etc/passwd //匹配包含27为数字开头的行，如27，277，2777... awk -F: &apos;$1~/mail/{print $1}&apos; /etc/passwd //$1匹配指定内容才显示 awk -F: &apos;{if($1~/mail/) print $1}&apos; /etc/passwd //与上面相同 awk -F: &apos;$1!~/mail/{print $1}&apos; /etc/passwd //不匹配 awk -F: &apos;$1!~/mail|mysql/{print $1}&apos; /etc/passwd IF语句 必须用在{}中，且比较内容用()扩起来 awk -F: &apos;{if($1~/mail/) print $1}&apos; /etc/passwd //简写 awk -F: &apos;{if($1~/mail/) {print $1}}&apos; /etc/passwd //全写 awk -F: &apos;{if($1~/mail/) {print $1} else {print $2}}&apos; /etc/passwd //if...else... 条件表达式 == != &gt; &gt;= awk -F&quot;:&quot; &apos;$1==&quot;mysql&quot;{print $3}&apos; /etc/passwd awk -F&quot;:&quot; &apos;{if($1==&quot;mysql&quot;) print $3}&apos; /etc/passwd //与上面相同 awk -F&quot;:&quot; &apos;$1!=&quot;mysql&quot;{print $3}&apos; /etc/passwd //不等于 awk -F&quot;:&quot; &apos;$3&gt;1000{print $3}&apos; /etc/passwd //大于 awk -F&quot;:&quot; &apos;$3&gt;=100{print $3}&apos; /etc/passwd //大于等于 awk -F&quot;:&quot; &apos;$3&lt;1{print $3}&apos; /etc/passwd //小于 awk -F&quot;:&quot; &apos;$3&lt;=1{print $3}&apos; /etc/passwd //小于等于 逻辑运算符 &amp;&amp; || awk -F: &apos;$1~/mail/ &amp;&amp; $3&gt;8 {print }&apos; /etc/passwd //逻辑与，$1匹配mail，并且$3&gt;8 awk -F: &apos;{if($1~/mail/ &amp;&amp; $3&gt;8) print }&apos; /etc/passwd awk -F: &apos;$1~/mail/ || $3&gt;1000 {print }&apos; /etc/passwd //逻辑或 awk -F: &apos;{if($1~/mail/ || $3&gt;1000) print }&apos; /etc/passwd 数值运算 awk -F: &apos;$3 &gt; 100&apos; /etc/passwd awk -F: &apos;$3 &gt; 100 || $3 &lt; 5&apos; /etc/passwd awk -F: &apos;$3+$4 &gt; 200&apos; /etc/passwd awk -F: &apos;/mysql|mail/{print $3+10}&apos; /etc/passwd //第三个字段加10打印 awk -F: &apos;/mysql/{print $3-$4}&apos; /etc/passwd //减法 awk -F: &apos;/mysql/{print $3*$4}&apos; /etc/passwd //求乘积 awk &apos;/MemFree/{print $2/1024}&apos; /proc/meminfo //除法 awk &apos;/MemFree/{print int($2/1024)}&apos; /proc/meminfo //取整 输出分隔符OFS awk &apos;$6 ~ /FIN/ || NR==1 {print NR,$4,$5,$6}&apos; OFS=&quot;\\t&quot; netstat.txt awk &apos;$6 ~ /WAIT/ || NR==1 {print NR,$4,$5,$6}&apos; OFS=&quot;\\t&quot; netstat.txt //输出字段6匹配WAIT的行，其中输出每行行号，字段4，5,6，并使用制表符分割字段 输出处理结果到文件 ①在命令代码块中直接输出 route -n|awk &apos;NR!=1{print &gt; &quot;./fs&quot;}&apos; ②使用重定向进行输出 route -n|awk &apos;NR!=1{print}&apos; &gt; ./fs 格式化输出 netstat -anp|awk &apos;{printf &quot;%-8s %-8s %-10s\\n&quot;,$1,$2,$3}&apos; printf表示格式输出 %格式化输出分隔符 -8长度为8个字符 s表示字符串类型 打印每行前三个字段，指定第一个字段输出字符串类型(长度为8)，第二个字段输出字符串类型(长度为8), 第三个字段输出字符串类型(长度为10) netstat -anp|awk &apos;$6==&quot;LISTEN&quot; || NR==1 {printf &quot;%-10s %-10s %-10s \\n&quot;,$1,$2,$3}&apos; netstat -anp|awk &apos;$6==&quot;LISTEN&quot; || NR==1 {printf &quot;%-3s %-10s %-10s %-10s \\n&quot;,NR,$1,$2,$3}&apos; IF语句 awk -F: &apos;{if($3&gt;100) print &quot;large&quot;; else print &quot;small&quot;}&apos; /etc/passwd small small small large small small awk -F: &apos;BEGIN{A=0;B=0} {if($3&gt;100) {A++; print &quot;large&quot;} else {B++; print &quot;small&quot;}} END{print A,&quot;\\t&quot;,B}&apos; /etc/passwd //ID大于100,A加1，否则B加1 awk -F: &apos;{if($3&lt;100) next; else print}&apos; /etc/passwd //小于100跳过，否则显示 awk -F: &apos;BEGIN{i=1} {if(i&lt;NF) print NR,NF,i++ }&apos; /etc/passwd awk -F: &apos;BEGIN{i=1} {if(i&lt;NF) {print NR,NF} i++ }&apos; /etc/passwd 另一种形式 awk -F: &apos;{print ($3&gt;100 ? &quot;yes&quot;:&quot;no&quot;)}&apos; /etc/passwd awk -F: &apos;{print ($3&gt;100 ? $3&quot;:\\tyes&quot;:$3&quot;:\\tno&quot;)}&apos; /etc/passwd while语句 awk -F: &apos;BEGIN{i=1} {while(i&lt;NF) print NF,$i,i++}&apos; /etc/passwd 7 root 1 7 x 2 7 0 3 7 0 4 7 root 5 7 /root 6 数组 netstat -anp|awk &apos;NR!=1{a[$6]++} END{for (i in a) print i,&quot;\\t&quot;,a[i]}&apos; netstat -anp|awk &apos;NR!=1{a[$6]++} END{for (i in a) printf &quot;%-20s %-10s %-5s \\n&quot;, i,&quot;\\t&quot;,a[i]}&apos; 9523 1 9929 1 LISTEN 6 7903 1 3038/cupsd 1 7913 1 10837 1 9833 1 应用1 awk -F: &apos;{print NF}&apos; helloworld.sh //输出文件每行有多少字段 awk -F: &apos;{print $1,$2,$3,$4,$5}&apos; helloworld.sh //输出前5个字段 awk -F: &apos;{print $1,$2,$3,$4,$5}&apos; OFS=&apos;\\t&apos; helloworld.sh //输出前5个字段并使用制表符分隔输出 awk -F: &apos;{print NR,$1,$2,$3,$4,$5}&apos; OFS=&apos;\\t&apos; helloworld.sh //制表符分隔输出前5个字段，并打印行号 应用2 awk -F&apos;[:#]&apos; &apos;{print NF}&apos; helloworld.sh //指定多个分隔符: #，输出每行多少字段 awk -F&apos;[:#]&apos; &apos;{print $1,$2,$3,$4,$5,$6,$7}&apos; OFS=&apos;\\t&apos; helloworld.sh //制表符分隔输出多字段 应用3 awk -F&apos;[:#/]&apos; &apos;{print NF}&apos; helloworld.sh //指定三个分隔符，并输出每行字段数 awk -F&apos;[:#/]&apos; &apos;{print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12}&apos; helloworld.sh //制表符分隔输出多字段 应用4 计算/home目录下，普通文件的大小，使用KB作为单位 ls -l|awk &apos;BEGIN{sum=0} !/^d/{sum+=$5} END{print &quot;total size is:&quot;,sum/1024,&quot;KB&quot;}&apos; ls -l|awk &apos;BEGIN{sum=0} !/^d/{sum+=$5} END{print &quot;total size is:&quot;,int(sum/1024),&quot;KB&quot;}&apos; //int是取整的意思 应用5 统计netstat -anp 状态为LISTEN和CONNECT的连接数量分别是多少 netstat -anp|awk &apos;$6~/LISTEN|CONNECTED/{sum[$6]++} END{for (i in sum) printf &quot;%-10s %-6s %-3s \\n&quot;, i,&quot; &quot;,sum[i]}&apos; 应用6 统计/home目录下不同用户的普通文件的总数是多少？ ls -l|awk &apos;NR!=1 &amp;&amp; !/^d/{sum[$3]++} END{for (i in sum) printf &quot;%-6s %-5s %-3s \\n&quot;,i,&quot; &quot;,sum[i]}&apos; mysql 199 root 374 统计/home目录下不同用户的普通文件的大小总size是多少？ ls -l|awk &apos;NR!=1 &amp;&amp; !/^d/{sum[$3]+=$5} END{for (i in sum) printf &quot;%-6s %-5s %-3s %-2s \\n&quot;,i,&quot; &quot;,sum[i]/1024/1024,&quot;MB&quot;}&apos; 应用7 输出成绩表 awk &apos;BEGIN{math=0;eng=0;com=0;printf &quot;Lineno. Name No. Math English Computer Total\\n&quot;;printf &quot;------------------------------------------------------------\\n&quot;}{math+=$3; eng+=$4; com+=$5;printf &quot;%-8s %-7s %-7s %-7s %-9s %-10s %-7s \\n&quot;,NR,$1,$2,$3,$4,$5,$3+$4+$5} END{printf &quot;------------------------------------------------------------\\n&quot;;printf &quot;%-24s %-7s %-9s %-20s \\n&quot;,&quot;Total:&quot;,math,eng,com;printf &quot;%-24s %-7s %-9s %-20s \\n&quot;,&quot;Avg:&quot;,math/NR,eng/NR,com/NR}&apos; test0 [root@localhost home]# cat test0 Marry 2143 78 84 77 Jack 2321 66 78 45 Tom 2122 48 77 71 Mike 2537 87 97 95 Bob 2415 40 57 62 awk手册 http://www.chinaunix.net/old_jh/7/16985.html","categories":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"}]},{"title":"springCloud多模块打包时报错问题","slug":"springCloud多模块打包时报错问题","date":"2019-04-25T07:08:51.000Z","updated":"2019-04-25T07:41:12.510Z","comments":true,"path":"2019/04/25/springCloud多模块打包时报错问题/","link":"","permalink":"http://yoursite.com/2019/04/25/springCloud多模块打包时报错问题/","excerpt":"执行mvn clean package spring-boot:repackage，报错如下：123[ERROR] Failed to execute goal org.springframework.boot:spring-boot-maven-plugin:1.5.3.RELEASE:repackage (default) on project webapps-api-bid: Execution default of goal org.springframework.boot:spring-boot-maven-plugin:1.5.3.RELEASE: repackage failed: Unable to find main class 错误提示： repackage failed: Unable to find main class","text":"执行mvn clean package spring-boot:repackage，报错如下：123[ERROR] Failed to execute goal org.springframework.boot:spring-boot-maven-plugin:1.5.3.RELEASE:repackage (default) on project webapps-api-bid: Execution default of goal org.springframework.boot:spring-boot-maven-plugin:1.5.3.RELEASE: repackage failed: Unable to find main class 错误提示： repackage failed: Unable to find main class原因： 多模块打包时，如果项目模块包含common，core等模块，这些模块不需要启动，应把其打成不可执行包来使用那当我们在maven中有多重依赖时，应注意一点，Common打包出来的应该是不可执行的jar包，所以不要在Common的pom中定义spring-boot-maven-plugin插件。 项目 yixue（父类工程，定义各模块，指定模块依赖jar版本） |------------------------------ |--yixue-admin 后台用户注册 | |--yixue-course 后台视频管理 | |--yixue-commom common工具包，维护工具类，公共类 | |--yixue-ui web界面，请求跳转，拦截等 | |--yixue-eureka SpringCloud注册 解决方法： common项目中除了必要的依赖包以外，maven打包的插件不要再添加一遍了，因为这个SpringBoot插件会在Maven的package后进行二次打包，目的为了生成可执行jar包，如果C中定义了这个插件，会报错提示没有找到main函数 简单来说，如果你的root：`&lt;parent&gt;&lt;/parent&gt;`项目已经添加了`spring-boot-maven-plugin`插件，那么common就别依赖root了，自己包含一些必要的依赖包，之后别手动添加打包插件即可，如果打包还是失败的话，对root项目clean再install一下，之后应该没有什么问题了","categories":[{"name":"springCloud,springboot","slug":"springCloud-springboot","permalink":"http://yoursite.com/categories/springCloud-springboot/"}],"tags":[{"name":"springCloud,springboot","slug":"springCloud-springboot","permalink":"http://yoursite.com/tags/springCloud-springboot/"}]},{"title":"SQL分析函数场景实例","slug":"SQL分析函数场景实例","date":"2019-04-18T10:51:34.000Z","updated":"2019-05-15T08:22:28.824Z","comments":true,"path":"2019/04/18/SQL分析函数场景实例/","link":"","permalink":"http://yoursite.com/2019/04/18/SQL分析函数场景实例/","excerpt":"分析函数 用来排序的函数它和聚合函数的不同之处是对于每个组返回多行row_number() over() rank() over() dense_rank() over() first_value() over()","text":"分析函数 用来排序的函数它和聚合函数的不同之处是对于每个组返回多行row_number() over() rank() over() dense_rank() over() first_value() over() 语法: row_number() over([partition by xxx] [order by xxx]) 返回的是行信息，没有排名 rank ( ) over([partition by xxx] [order by xxx]) 返回的相关等级不会跳跃 dense_rank ( ) over([partition by xxx] [order by xxx]) 返回的返回的相关等级会跳跃 场景: 查询每个班的第一名的成绩 SELECT * FROM (select t.name,t.class,t.sroce,rank() over(partition by t.class order by t.sroce desc) mm from T2_TEMP t) where mm = 1; Lag Lead 语法: lag(exp_str,offset,defval) over(partion by ..order by …) lead(exp_str,offset,defval) over(partion by ..order by …) 其中exp_str是字段名 Offset是偏移量，即是上1个或上N个的值，假设当前行在表中排在第5行，则offset 为3，则表示我们所要找的数据行就是表中的第2行（即5-3=2）,默认值是1 Defval默认值，当两个函数取上N/下N个值，当在表中从当前行位置向前数N行已经超出了表的范围时，lag（）函数将defval这个参数值作为函数的返回值，若没有指定默认值，则返回NULL，那么在数学运算中，总要给一个默认值才不会出错。 聚合函数 聚合函数对于每个组只返回一行 count、max、min、sum、avg、Variance、Stddev Count 用来求有效数据的数量 Max 用来求给定数据中最大的那一个数据 Min 用来求给定数据中最小的那一个数据 Avg 用来求给定数据的平均值 Sum 用来求给定数据的总和 Variance 用来求给定数据的标准差 Stddev 用来求给定数据的方差 median 主要用于统计整表或者分组情况下的中位数（限定参数为数值型或日期/时间型），忽略NULL值 对于聚合函数，如果给定的值中存在空值的话，oracle将会直接忽略 select count(*) from xxx 对于聚合函数中可以使用distinct关键字来压缩重复值 比如我们想统计总共有多少个部门的话 我们如果写 Select count(deptno) from emp; 将会得到错误的结果。因为实际上有很多重复的值也被计算在内。为了找到正确的答案，你应该这样写。 Select count(distinct deptno) from emp; collect函数 collect_list collect_set 它们都是将分组中的某列转为一个数组返回，不同的是collect_list不去重而collect_set去重。 语法: collect_list(column_name) collect_set(column_name) 有的时候我们想根据A进行分组然后随便取出每个分组中的一个B，代入到这个实验中就是按照用户进行分组，然后随便拿出一个他看过的视频名称即可 collect_list(column_name)[0] 和取数组一样简单 ###","categories":[{"name":"sql","slug":"sql","permalink":"http://yoursite.com/categories/sql/"}],"tags":[{"name":"sql","slug":"sql","permalink":"http://yoursite.com/tags/sql/"}]},{"title":"springboot踩坑出坑记","slug":"springboot踩坑出坑记","date":"2019-04-17T09:57:26.000Z","updated":"2019-04-25T07:57:20.310Z","comments":true,"path":"2019/04/17/springboot踩坑出坑记/","link":"","permalink":"http://yoursite.com/2019/04/17/springboot踩坑出坑记/","excerpt":"4月15到4月17我都在把毕设从eclipse重构到IDEA中，springboot最让我头疼的是它的版本问题，因为每一个版本对应的依赖包都有可能出错，这里分享一下如何成功移植用eclipse写的springboot到IDEA中，比较简单的步骤我这里不详细说了，说一下我遇到的一些很难找出问题的地方ps:只是针对于我的项目和我个人水平，大神勿喷嘿嘿","text":"4月15到4月17我都在把毕设从eclipse重构到IDEA中，springboot最让我头疼的是它的版本问题，因为每一个版本对应的依赖包都有可能出错，这里分享一下如何成功移植用eclipse写的springboot到IDEA中，比较简单的步骤我这里不详细说了，说一下我遇到的一些很难找出问题的地方ps:只是针对于我的项目和我个人水平，大神勿喷嘿嘿 springboot-mybatis整合坑 出现下方错误请查看启动类：XXXApplication 是否扫描到mapper映射文件，声明eclipse和idea不一样，这里eclipse可以跑通，idea中不行1234567891011121314151617181920***************************APPLICATION FAILED TO START***************************Description:Field chapterDao in cn.yixue.service.ChapterServiceImp required a bean of type &apos;cn.yixue.dao.ChapterMapper&apos; that could not be found.The injection point has the following annotations: - @org.springframework.beans.factory.annotation.Autowired(required=true)Action:Consider defining a bean of type &apos;cn.yixue.dao.ChapterMapper&apos; in your configuration.以上提取出有用的信息：required a bean of type &apos;xxxx&apos; that could not be found.代表bean没注入，从bean注入寻找方向，有的人会说我用@Autowired之类的种种，但没扫到，好吧~ 解决方法： 在相应的mapper类中加@Mapper标注让springboot根据标注去将mapper注入 1234@Mapperpublic interface ChapterMapper &#123; ......&#125; 启动类加@MapperScan(value = &quot;cn.yixue.video.dao&quot;) value 后的包一定要对应到mapper类对应的地方，比如我的mapper在dao下，就是cn.yixue.video.dao 12345678910@SpringBootApplication@MapperScan(value = &quot;cn.yixue.video.dao&quot;)@EnableDiscoveryClientpublic class YixueVideoApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(YixueVideoApplication.class, args); &#125;&#125; Spring Boot项目中含有Mybatis,打Jar包运行之后,报如下错误: 12345678910111213141516***************************APPLICATION FAILED TO START***************************Description:Failed to configure a DataSource: &apos;url&apos; attribute is not specified and no embedded datasource could be configured.Reason: Failed to determine a suitable driver classAction:Consider the following: If you want an embedded database (H2, HSQL or Derby), please put it on the classpath. If you have database settings to be loaded from a particular profile you may need to activate it (no profiles are currently active). 网上好多解决方案，针对于每个人都不一样，我的应该是打包的时候读不到我的配置文件，需要在pom.xml里面加resourses指定下配置文件，因为eclipse是识别的，Idea可能不会？我也不太知道，反正是加上了，因为好像有Idea读不到我的application.properties或者application.yml文件，我就一次性都配上了，这个大家具体遇到的时候再去搜一下就行，不用刻意的记: 1234567891011121314151617181920212223&lt;build&gt;&lt;!-- 如果不添加此节点mybatis的mapper.xml文件都会被漏掉。 --&gt;&lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.yml&lt;/include&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.yml&lt;/include&gt; &lt;include&gt;**/*.properties&lt;/include&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;filtering&gt;false&lt;/filtering&gt; &lt;/resource&gt;&lt;/resources&gt;&lt;/build&gt; springBoot-SpringCloud整合坑 利用SpringCloud做服务注册时，Eclipse需要自己导jar包依赖和配置版本，Idea直接可以再创建Springboot项目时鼠标点击引入，这个我就放几张图来解释： 最后一个next后直接finish…… 之后再pom.xml里面会看到Idea自动为你引入的依赖和 spring-boot-maven-plugin 插件，插件版本我建议还是稍微低一点，因为boot真的是随着版本变动改动很大，我用的是12345&lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt;&lt;/plugin&gt; 这个也是在网上搜了很久之后找到的一个版本，还有一个1.4.9.RELEASE也可以，之后就是看看Idea导入的SpringCloud依赖的版本version，版本错误很容易报java.lang.AbstractMethodError: null这个错误我找了很久，原因也是看了一个大佬的博客找到的，具体就是因为Idea给你的依赖是根据你选择的springboot的版本来的，一般人不会去修改，这也就是为什么eclipse不容易报错，Idea容易的原因，因为eclipse得自己找…123456&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.5.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;&lt;/parent&gt; 我的parent的版本是&lt;version&gt;2.0.5.RELEASE&lt;/version&gt;，大佬的文章也提到了2.1.0和2.1.0以下的有区别，我还是建议用低的，低的差别不会太大，还挺稳……我还用过1.5.9.RELEASE… 之后配置eureka的服务的时候Idea提供的版本也要改，这个原因是因为如果使用${spring-cloud.version}的话,当版本号下调到2.1.0以下的时候,一些组件的包还是2.1.0它不会跟随parent版本的下调而下调,也就是parent的版本小于组件的版本,这时候就会出问题当改为Finchley.RELEASE的时候,组件的依赖就会跟随parent的版本下调而下调123456789101112131415161718&lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;!-- 这是Idea给设的 --&gt; &lt;!--&lt;spring-cloud.version&gt;Greenwich.SR1&lt;/spring-cloud.version&gt;--&gt; &lt;spring-cloud.version&gt;Finchley.RELEASE&lt;/spring-cloud.version&gt;&lt;/properties&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-cloud.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 解决静态资源跨域时的坑 之前在eclipse中静态资源访问是可以通过12@Autowiredprivate RestTemplate restTemplate; 直接装配的，之后到了Idea中报错了:123456789101112131415ERROR 31473 --- [ main] o.s.b.d.LoggingFailureAnalysisReporter : ***************************APPLICATION FAILED TO START***************************Description:Field restTemplate in &apos;xxxxxx&apos;required a bean of type &apos;org.springframework.web.client.RestTemplate&apos; that could not be found.Action:Consider defining a bean of type &apos;org.springframework.web.client.RestTemplate&apos; in your configuration. 解决方法如下，大致就是先靠@Bean装配，再用…:1234567@Beanpublic RestTemplate restTemplate(RestTemplateBuilder builder) &#123; // Do any additional configuration here return builder.build();&#125;@Autowiredprivate RestTemplate restTemplate; 之后就不报错了(针对于我的错误) 我的 pom.xml(project的xml)我的架构 圈住的地方是下方的pom.xml文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.yixue&lt;/groupId&gt; &lt;artifactId&gt;yixue&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.5.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;lombok.version&gt;1.14.8&lt;/lombok.version&gt; &lt;fastjson.version&gt;1.2.31&lt;/fastjson.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.46&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.2.5&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;$&#123;lombok.version&#125;&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;$&#123;fastjson.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;configuration&gt; &lt;!--该配置必须--&gt; &lt;fork&gt;true&lt;/fork&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;modules&gt; &lt;module&gt;yixue-commom&lt;/module&gt; &lt;module&gt;yixue-admin&lt;/module&gt; &lt;module&gt;yixue-video&lt;/module&gt; &lt;/modules&gt;&lt;/project&gt; 这就是我移植项目遇到的一些问题，下面列一些大佬的博客，对我帮助很大，不胜感激如有侵权，请联系我删除 springboot @WebFilter过滤器的使用 java.lang.AbstractMethodError: null 不能自动装配RestTemplate / not found SpringBoot 2.0 报错: Failed to configure a DataSource: ‘url’ attribute is not specified and no embe… 在pom.xml文件中使用resources插件的小作用","categories":[{"name":"springCloud,springboot","slug":"springCloud-springboot","permalink":"http://yoursite.com/categories/springCloud-springboot/"}],"tags":[{"name":"springCloud,springboot","slug":"springCloud-springboot","permalink":"http://yoursite.com/tags/springCloud-springboot/"}]},{"title":"含有重复字符的最长子串","slug":"含有重复字符的最长子串","date":"2019-04-10T08:25:25.000Z","updated":"2019-04-10T10:03:49.705Z","comments":true,"path":"2019/04/10/含有重复字符的最长子串/","link":"","permalink":"http://yoursite.com/2019/04/10/含有重复字符的最长子串/","excerpt":"* 给定一个字符串，请你找出其中不含有重复字符的最长子串的长度。 * * 示例 1: * 输入: &quot;abcabcbb&quot; * 输出: 3 * 解释: 因为无重复字符的最长子串是 &quot;abc&quot;，所以其长度为 3。","text":"* 给定一个字符串，请你找出其中不含有重复字符的最长子串的长度。 * * 示例 1: * 输入: &quot;abcabcbb&quot; * 输出: 3 * 解释: 因为无重复字符的最长子串是 &quot;abc&quot;，所以其长度为 3。 * &lt;p&gt; * 示例 2: * &lt;p&gt; * 输入: &quot;bbbbb&quot; * 输出: 1 * 解释: 因为无重复字符的最长子串是 &quot;b&quot;，所以其长度为 1。 * &lt;p&gt; * 示例 3: * &lt;p&gt; * 输入: &quot;pwwkew&quot; * 输出: 3 * 解释: 因为无重复字符的最长子串是 &quot;wke&quot;，所以其长度为 3。 * 请注意，你的答案必须是 子串 的长度，&quot;pwke&quot; 是一个子序列，不是子串。 */ 12345678910111213141516171819202122232425public class LongestSubstring &#123; public static int lengthOfLongestSubstring(String s) &#123; int length = s.length(); if(length&lt;2) &#123;return length;&#125; Set set = new HashSet(); int res = 0; int start = 0; int end = 0; while (end&lt;length)&#123; if(!set.contains(s.charAt(end)))&#123; set.add(s.charAt(end++)); res = Math.max(res,set.size()); &#125;else&#123; set.remove(s.charAt(start++)); &#125; &#125; return res; &#125; public static void main(String[] args) &#123; int result = lengthOfLongestSubstring(&quot;abcabcbb&quot;); System.out.println(result); &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://yoursite.com/tags/leetcode/"}]},{"title":"慢SQL优化解决方案(看这一篇就够了)","slug":"慢SQL优化解决方案-看这一篇就够了","date":"2019-03-25T02:51:08.000Z","updated":"2019-04-17T10:57:22.569Z","comments":true,"path":"2019/03/25/慢SQL优化解决方案-看这一篇就够了/","link":"","permalink":"http://yoursite.com/2019/03/25/慢SQL优化解决方案-看这一篇就够了/","excerpt":"慢SQL问题导致慢 SQL 的原因：在遇到慢 SQL 情况时，不能简单的把原因归结为 SQL 编写问题(虽然这是最常见的因素)实际上导致慢 SQL 有很多因素，甚至包括硬件和 mysql 本身的 bug。根据出现的概率从大到小，罗列如下：","text":"慢SQL问题导致慢 SQL 的原因：在遇到慢 SQL 情况时，不能简单的把原因归结为 SQL 编写问题(虽然这是最常见的因素)实际上导致慢 SQL 有很多因素，甚至包括硬件和 mysql 本身的 bug。根据出现的概率从大到小，罗列如下： SQL编写问题 锁 业务实例相互干绕对 IO/CPU 资源争用 服务器硬件 MYSQL BUG 由 SQL 编写导致的慢 SQL 优化 针对SQL编写导致的慢 SQL，优化起来还是相对比较方便的。正如上一节提到的正确的使用索引能加快查询速度，那么我们在编写 SQL 时就需要注意与索引相关的规则： 1.字段类型转换导致不用索引，如字符串类型的不用引号，数字类型的用引号等，这有可能会用不到索引导致全表扫描； 2.mysql 不支持函数转换，所以字段前面不能加函数，否则这将用不到索引； 3.不要在字段前面加减运算； 4.字符串比较长的可以考虑索引一部份减少索引文件大小，提高写入效率； 5.like % 在前面用不到索引； 6.根据联合索引的第二个及以后的字段单独查询用不到索引； 7.不要使用 select *； 8.排序请尽量使用升序 ; 9.or 的查询尽量用 union 代替 （Innodb）； 10.复合索引高选择性的字段排在前面； 11.order by / group by 字段包括在索引当中减少排序，效率会更高。 12.除了上述索引使用规则外，SQL 编写时还需要特别注意一下几点： 13.尽量规避大事务的 SQL，大事务的 SQL 会影响数据库的并发性能及主从同步； 14.分页语句 limit 的问题； 15.删除表所有记录请用 truncate，不要用 delete； 16.不让 mysql 干多余的事情，如计算； 17.输写 SQL 带字段，以防止后面表变更带来的问题，性能也是比较优的 ( 涉及到数据字典解析，请自行查询资料)； 18.在 Innodb上用 select count(*)，因为 Innodb 会存储统计信息； 19.慎用 Oder by rand()。 分析诊断工具在日常开发工作中，我们可以做一些工作达到预防慢 SQL 问题，比如在上线前预先用诊断工具对 SQL 进行分析。常用的工具有： mysqldumpslow mysql profile mysql explain 具体使用及分析方法在此就不赘述，网上有丰富的资源可以参考。 误操作、程序 bug 时怎么办 提出这个问题显然主要是针对刚开始工作的年轻同行们……实际上误操作和程序 bug 导致数据误删或者混乱的问题并非少见 但是刚入行的开发工作者会比较紧张。一个成熟的企业往往会有完善的数据管理规范和较丰富的数据恢复方案（初创公司除外） 会进行数据备份和数据容灾。当你发现误操作或程序 bug 导致线上数据被误删或误改动时，一定不能慌乱，应及时与 DBA 联系 第一时间进行数据恢复（严重时直接停止服务），尽可能减少影响和损失。对于重要数据（如资金）的操作，在开发时一定要反复进行测试，确保没有问题后再上线。","categories":[],"tags":[{"name":"SQL","slug":"SQL","permalink":"http://yoursite.com/tags/SQL/"}]},{"title":"字符移位问题","slug":"字符移位问题","date":"2019-03-20T09:12:41.000Z","updated":"2019-04-10T08:36:23.749Z","comments":true,"path":"2019/03/20/字符移位问题/","link":"","permalink":"http://yoursite.com/2019/03/20/字符移位问题/","excerpt":"有一个由小写字母组成的字符串 S，和一个整数数组 shifts。我们将字母表中的下一个字母称为原字母的 移位（由于字母表是环绕的， ‘z’ 将会变成 ‘a’）。例如·，shift(‘a’) = ‘b’， shift(‘t’) = ‘u’,， 以及 shift(‘z’) = ‘a’。对于每个 shifts[i] = x ， 我们会将 S 中的前 i+1 个字母移位 x 次。返回将所有这些移位都应用到 S 后最终得到的字符串。","text":"有一个由小写字母组成的字符串 S，和一个整数数组 shifts。我们将字母表中的下一个字母称为原字母的 移位（由于字母表是环绕的， ‘z’ 将会变成 ‘a’）。例如·，shift(‘a’) = ‘b’， shift(‘t’) = ‘u’,， 以及 shift(‘z’) = ‘a’。对于每个 shifts[i] = x ， 我们会将 S 中的前 i+1 个字母移位 x 次。返回将所有这些移位都应用到 S 后最终得到的字符串。 字符移位问题 leetcode 843/** * 字符移位问题 leetcode 843 * 有一个由小写字母组成的字符串 S，和一个整数数组 shifts。 * 我们将字母表中的下一个字母称为原字母的 移位（由于字母表是环绕的， &apos;z&apos; 将会变成 &apos;a&apos;）。 * 例如·，shift(&apos;a&apos;) = &apos;b&apos;， shift(&apos;t&apos;) = &apos;u&apos;,， 以及 shift(&apos;z&apos;) = &apos;a&apos;。 * 对于每个 shifts[i] = x ， 我们会将 S 中的前 i+1 个字母移位 x 次。 * 返回将所有这些移位都应用到 S 后最终得到的字符串。 * * 示例： * 输入：S = &quot;abc&quot;, shifts = [3,5,9] * 输出：&quot;rpl&quot; * 解释： * 我们以 &quot;abc&quot; 开始。 * 将 S 中的第 1 个字母移位 3 次后，我们得到 &quot;dbc&quot;。 * 再将 S 中的前 2 个字母移位 5 次后，我们得到 &quot;igc&quot;。 * 最后将 S 中的这 3 个字母移位 9 次后，我们得到答案 &quot;rpl&quot;。 */ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public class shiftingLetters &#123; /** * 方法一：不推荐，但好理解 * @author yiming.liang * @date 2019/3/20 15:33 * @param [S, shifts] * @return java.lang.String */ public static String shiftingLetter(String S, int[] shifts) &#123; //目的为了拼字符串的 StringBuffer sb = new StringBuffer(); char c = 0; for(int i = shifts.length-2;i&gt;=0;i--)&#123; //从后往前算应该向后走几步,因为题目中说第二个走，第一个也走，第三个走，前两个也走 //所以最后一个走的是最少的，只走一步，而第一个走的最多，是后几个步数的和 //这样就比较好理解，我们直接从倒数第二个字母开始，将他和最后一人走的加起来，算出倒数第二个字母走的步数 //之后依次累加即可 shifts[i] += shifts[i+1]%26; &#125; for (int i = 0; i &lt; S.length(); i++) &#123;// c = (char)(c.charAt(i) - &apos;a&apos; + shift) % 26 + &apos;a&apos;); //这里分情况，上面那个是最好的方式但难理解，下面那个是好理解但麻烦 //下面这边分两种情况，第一钟是字母向后走但不超过z 直接加即可 c = (char)(S.charAt(i) + shifts[i]%26); //如果超了z即122，这里拿一个栗子来说明,用例中&quot;ruu&quot;，&#123;26,9,17&#125;; //u要向后走17步，而u的ascii码为117，走17步明显超过了z则判断是否大于122 //如果大于122，则算出大了多少，17+117-122 = 12则我们从u开始还要向后走5步到z //再返回去走12步，这时我们是从z开始走的，则a也要被算在内，而我们后面的+&apos;a&apos;则代表着从a开始走 //显然a也被算在这12步内了，所以要-1再+&apos;a&apos; //over if(c&gt;122)&#123; c = (char) ((c-122)-1 + &apos;a&apos;); sb.append(c); &#125;else&#123; sb.append(c); &#125; &#125; return sb.toString(); &#125; /** * 方法二:推荐 * @author yiming.liang * @date 2019/3/20 16:45 * @param [S, shifts] * @return java.lang.String */ public String shiftingLetter2(String S, int[] shifts) &#123; char[] arr = S.toCharArray(); int shift = 0; for (int i = arr.length - 1; i &gt;= 0; i--) &#123; shift = (shift + shifts[i]) % 26; arr[i] = (char)((arr[i] - &apos;a&apos; + shift) % 26 + &apos;a&apos;); &#125; return new String(arr); &#125; public static void main(String[] args) &#123;// char i = &apos;a&apos;+1; int[] shifts = &#123;26,9,17&#125;; String str = shiftingLetter(&quot;ruu&quot;,shifts); System.out.println(str); &#125;&#125;","categories":[{"name":"算法","slug":"算法","permalink":"http://yoursite.com/categories/算法/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://yoursite.com/tags/leetcode/"}]},{"title":"leetcode 1013:总持续时间可被60整除的歌曲","slug":"leetcode-1013-总持续时间可被-60-整除的歌曲","date":"2019-03-20T07:03:42.000Z","updated":"2019-03-20T07:14:34.504Z","comments":true,"path":"2019/03/20/leetcode-1013-总持续时间可被-60-整除的歌曲/","link":"","permalink":"http://yoursite.com/2019/03/20/leetcode-1013-总持续时间可被-60-整除的歌曲/","excerpt":"","text":"leetcode 1013:总持续时间可被 60 整除的歌曲 在歌曲列表中，第 i 首歌曲的持续时间为 time[i] 秒。返回其总持续时间（以秒为单位）可被 60 整除的歌曲对的数量。形式上，我们希望索引的数字 i &lt; j 且有 (time[i] + time[j]) % 60 == 0。 示例 1： 输入：[30,20,150,100,40] 输出：3 解释：这三对的总持续时间可被 60 整数： (time[0] = 30, time[2] = 150): 总持续时间 180 (time[1] = 20, time[3] = 100): 总持续时间 120 (time[1] = 20, time[4] = 40): 总持续时间 60 示例 2： 输入：[60,60,60] 输出：3 解释：所有三对的总持续时间都是 120，可以被 60 整数。 12345678910111213方法1：时间复杂度O(n^2)，效率奇低class Solution &#123; public int numPairsDivisibleBy60(int[] time) &#123; int resutl = 0; for(int i = 0;i&lt;time.length-1;i++)&#123; for (int j = i+1; j &lt; time.length; j++) &#123; if(time[i]+time[j]%60==0)&#123; result++; &#125; &#125; &#125; &#125;&#125; 1234567891011121314151617181920212223方法2： 利用数组，将每个数对60取模后的值当作新数组的下标，并记录次数 接着遇到连续多个都能整出60的数，则此时 arr[0]的值应该会++多次 若arr[0]=3 即有3个60，他们两两组合则可以组合的数量为(n(n-1))/2 result = (3*2)/2 = 3 同理，取mode后的值为30也是特殊情况，则组合的方法和0的一样，接着下面写就可以了 排除了特殊情况后，正常情况就是20 40, 10 50之类的相加为60的数，这些数2 2 组合直接乘就可以了 class Solution &#123; public int numPairsDivisibleBy60(int[] time) &#123; int result = 0; int[] res = new int[60]; for(int i:time)&#123; res[i%60]++; &#125; result += (res[0] * (res[0]-1))/2; result += (res[30] * (res[30]-1))/2; //正常情况，相加为60证明一定能被60整除 for(int i = 1;i&lt;30;i++)&#123; result += (res[i] * res[60-i]); &#125; return result; &#125;&#125;","categories":[{"name":"JAVA","slug":"JAVA","permalink":"http://yoursite.com/categories/JAVA/"}],"tags":[{"name":"Leetcode","slug":"Leetcode","permalink":"http://yoursite.com/tags/Leetcode/"}]},{"title":"Kafka如何利用Zookeeper做负载均衡","slug":"Kafka如何利用Zookeeper做负载均衡","date":"2019-03-20T03:01:08.000Z","updated":"2019-03-20T03:02:07.958Z","comments":true,"path":"2019/03/20/Kafka如何利用Zookeeper做负载均衡/","link":"","permalink":"http://yoursite.com/2019/03/20/Kafka如何利用Zookeeper做负载均衡/","excerpt":"","text":"Kafka与Zookeeper Kafka如何利用zookeeper做负载均衡 Kafka使用zk的分布式协调服务，将生产者，消费者，消息储存（broker，用于存储信息，消息读写等）结合在一起。同时借助zk，kafka能够将生产者，消费者和broker在内的所有组件在无状态的条件下建立起生产者和消费者的订阅关系，实现生产者的负载均衡。 broker在zk中注册 kafka的每个broker（相当于一个节点，相当于一个机器）在启动时，都会在zk中注册，告诉zk其brokerid，在整个的集群中，broker.id/brokers/ids，当节点失效时，zk就会删除该节点，就很方便的监控整个集群broker的变化，及时调整负载均衡。 topic在zk中注册 在kafka中可以定义很多个topic，每个topic又被分为很多个分区。一般情况下，每个分区独立在存在一个broker上，所有的这些topic和broker的对应关系都有zk进行维护 consumer(消费者)在zk中注册 注册新的消费者，当有新的消费者注册到zk中，zk会创建专用的节点来保存相关信息，路径ls /consumers/{group_id}/ [ids,owners,offset]，Ids:记录该消费分组有几个正在消费的消费者，Owmners：记录该消费分组消费的topic信息，Offset：记录topic每个分区中的每个offset 监听消费者分组中消费者的变化 ,监听/consumers/{group_id}/ids的子节点的变化，一旦发现消费者新增或者减少及时调整消费者的负载均衡。 转载 作者：SmartBrain 原文：https://blog.csdn.net/Peter_Changyb/article/details/81562855","categories":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/categories/kafka/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/tags/kafka/"}]},{"title":"maven之pom.xml中的元素解析","slug":"maven之pom-xml中的元素解析","date":"2019-03-19T12:14:44.000Z","updated":"2019-04-12T06:03:32.436Z","comments":true,"path":"2019/03/19/maven之pom-xml中的元素解析/","link":"","permalink":"http://yoursite.com/2019/03/19/maven之pom-xml中的元素解析/","excerpt":"","text":"Maven-pom.xml-中元素解析modules模块 用处：项目规模比较大，模块较为复杂，目的是为了聚合，一次性构建全部模块 123456代码：&lt;modules&gt; &lt;!-- 模块都写在此处 --&gt; &lt;module&gt;admin-register&lt;/module&gt; &lt;module&gt;admin-login&lt;/module&gt;&lt;/modules&gt; dependencyManagement依赖管理 用处：管理maven依赖 12345678代码：&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.yixue.sms&lt;/groupId&gt; &lt;artifactId&gt;sms-dubbo-api&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; parent继承 用处：类似与java中的继承，如果每个子模块都用了相同的依赖包，则配置父模块，子模块继承父模块则代表继承了父模块的依赖包 1234567891011121314151617181920212223242526272829303132代码：父：&lt;modules&gt; &lt;!-- 模块都写在此处 --&gt; &lt;module&gt;admin-register&lt;/module&gt; &lt;module&gt;admin-login&lt;/module&gt;&lt;/modules&gt;&lt;dependencies&gt; &lt;!-- 配置共有依赖 --&gt; &lt;!-- spring 依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;4.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;4.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;4.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context-support&lt;/artifactId&gt; &lt;version&gt;4.0.2.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 12345678910111213141516171819202122子：&lt;parent&gt; &lt;groupId&gt;com.admin.user&lt;/groupId&gt; &lt;artifactId&gt;admin-login&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../pom.xml&lt;/relativePath&gt; &lt;!-- 与不配置一样，默认就是寻找上级目录下得pom.xml --&gt;&lt;/parent&gt;&lt;!-- 配置自己独有依赖 --&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;javax.mail&lt;/groupId&gt; &lt;artifactId&gt;mail&lt;/artifactId&gt; &lt;version&gt;1.4.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.icegreen&lt;/groupId&gt; &lt;artifactId&gt;greenmail&lt;/artifactId&gt; &lt;version&gt;1.4.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 假设将来需要添加一个新的子模块admin-util，该模块只是提供一些简单的帮助工具，不需要依赖spring那么我们可以用dependencyManagement既能让子模块继承到父模块的依赖配置，又能保证子模块依赖使用的灵活性 在dependencyManagement元素下得依赖声明不会引入实际的依赖，不过它能够约束dependencies下的依赖使用 父POM使用dependencyManagement能够统一项目范围中依赖的版本 当依赖版本在父POM中声明后，子模块在使用依赖的时候就无须声明版本，也就不会发生多个子模块使用版本不一致的情况，帮助降低依赖冲突的几率 properties自定义一个或者多个Maven属性，然后再POM的其他地方使用${属性名}的方式引用该属性 作用：消除重复，统一管理 123456用法&lt;properties&gt; &lt;!-- 定义 spring版本号 --&gt; &lt;spring.version&gt;4.0.2.RELEASE&lt;/spring.version&gt; &lt;junit.version&gt;4.7&lt;/junit.version&gt;&lt;/properties&gt; 12345678910111213141516171819202122232425这里我记录了一些pom的属性：用户可以使用该类属性引用POM文件中对应元素的值。如： $&#123;project.artifactId&#125;就对应了&lt;project&gt; &lt;artifactId&gt;元素的值，常用的POM属性包括： $&#123;project.build.sourceDirectory&#125;:项目的主源码目录，默认为src/main/java/ $&#123;project.build.testSourceDirectory&#125;:项目的测试源码目录，默认为src/test/java/ $&#123;project.build.sourceEncoding&#125;表示主源码的编码格式; $&#123;project.build.directory&#125; ： 项目构建输出目录，默认为target/ $&#123;project.outputDirectory&#125; : 项目主代码编译输出目录，默认为target/classes/ $&#123;project.testOutputDirectory&#125;：项目测试主代码输出目录，默认为target/testclasses/ $&#123;project.groupId&#125;：项目的groupId $&#123;project.artifactId&#125;：项目的artifactId $&#123;project.version&#125;：项目的version,与$&#123;version&#125; 等价 $&#123;project.build.finalName&#125;：项目打包输出文件的名称，默认为$&#123;project.artifactId&#125;-$&#123;project.version&#125; Exclusions排除依赖 用法：来排除一些不需要同时下载的依赖jar 举例：B项目中需要导入A项目的Maven依赖，通过依赖传递，会将A中的Jar包传递进来，如果B中不需要A中的某个jar包就可以使用exclusions标签 1234567891011用法：&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; 环境变量属性所有环境变量属性都可以使用以env. 开头的Maven属性引用，如${env.JAVA_HOME}指代了JAVA_HOME环境变量的的值 beta:软件的验收测试 如果配置了&lt;env&gt;beta&lt;/env&gt; ${env} = beta 内置属性(Maven预定义,用户可以直接使用) ${basedir}表示项目根目录,即包含pom.xml文件的目录; ${version}表示项目版本; ${project.basedir}同${basedir}; ${project.baseUri}表示项目文件地址; ${maven.build.timestamp}表示项目构件开始时间; ${maven.build.timestamp.format}表示属性${maven.build.timestamp}的展示格式,默认值为yyyyMMdd-HHmm,可自定义其格式,其类型可参考java.text.SimpleDateFormat。用法如下： 123&lt;properties&gt;&lt;maven.build.timestamp.format&gt;yyyy-MM-dd HH:mm:ss&lt;/maven.build.timestamp.format&gt;&lt;/properties&gt; build在Maven的pom.xml文件中，存在如下两种&lt;build&gt;： （1）全局配置（project build） 针对整个项目的所有情况都有效 （2）配置（profile build） 针对不同的profile配置 共用的基本build元素: defaultGoal，执行构建时默认的goal或phase，如jar:jar或者package等 directory，构建的结果所在的路径，默认为${basedir}/target目录 finalName，构建的最终结果的名字，该名字可能在其他plugin中被改变 &lt;build&gt; &lt;filters&gt; &lt;filter&gt;../config/filters-${env}.properties&lt;/filter&gt; &lt;/filters&gt; &lt;resources&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources/&lt;/directory&gt; &lt;excludes&gt; &lt;exclude&gt;test/*&lt;/exclude&gt; &lt;exclude&gt;beta/*&lt;/exclude&gt; &lt;exclude&gt;online/*&lt;/exclude&gt; &lt;/excludes&gt; &lt;!-- 是否使用过滤器 --&gt; &lt;filtering&gt;true&lt;/filtering&gt; &lt;/resource&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources/${profiles.active}&lt;/directory&gt; &lt;/resource&gt; &lt;/resources&gt; &lt;/build&gt; resources，build过程中涉及的资源文件 targetPath，资源文件的目标路径 filtering，构建过程中是否对资源进行过滤，默认false directory，资源文件的路径，默认位于${basedir}/src/main/resources/目录下 includes，一组文件名的匹配模式，被匹配的资源文件将被构建过程处理 excludes，一组文件名的匹配模式，被匹配的资源文件将被构建过程忽略。同时被includes和excludes匹配的资源文件，将被忽略。 filters，给出对资源文件进行过滤的属性文件的路径，默认位于${basedir}/src/main/filters/目录下。属性文件中定义若干键值对。在构建过程中，对于资源文件中出现的变量（键），将使用属性文件中该键对应的值替换。 testResources，test过程中涉及的资源文件，默认位于${basedir}/src/test/resources/目录下。这里的资源文件不会被构建到目标构件中","categories":[{"name":"maven","slug":"maven","permalink":"http://yoursite.com/categories/maven/"}],"tags":[{"name":"maven","slug":"maven","permalink":"http://yoursite.com/tags/maven/"}]},{"title":"Leetcode 177 第N高的薪水","slug":"Leetcode-177-第N高的薪水","date":"2019-02-22T13:35:20.000Z","updated":"2019-02-22T13:45:28.982Z","comments":true,"path":"2019/02/22/Leetcode-177-第N高的薪水/","link":"","permalink":"http://yoursite.com/2019/02/22/Leetcode-177-第N高的薪水/","excerpt":"","text":"编写一个 SQL 查询，获取 Employee 表中第 n 高的薪水（Salary）。 测试数据 +----+--------+ | Id | Salary | +----+--------+ | 1 | 100 | | 2 | 200 | | 3 | 300 | +----+--------+ 例如上述 Employee 表，n = 2 时，应返回第二高的薪水 200。如果不存在第 n 高的薪水，那么查询应返回 null。 +------------------------+ | getNthHighestSalary(2) | +------------------------+ | 200 | +------------------------+ 首先拿到题直接思考使用分析函数了,是使用row_number()还是dense_rank() row_number() over()是无论值是否相同值始终递增 100 1 200 2 200 3 300 4 dense_rank() over()是重复的值显示的排名都相同，但其后的值跟着上一个值递增，不跳跃 100 1 200 2 200 2 300 3 rank() over()则是重复值的排名相同，但其后的值从重复行数开始递增，跳跃式 100 1 200 2 200 2 300 4 题解 CREATE FUNCTION getNthHighestSalary(N IN NUMBER) RETURN NUMBER IS result NUMBER; BEGIN /* Write your PL/SQL query statement below */ SELECT a.salary into result from ( select dense_rank() over(order by Salary desc) as rank,Salary from Employee )a where a.rank = N; RETURN result; END;","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/数据库/"}],"tags":[{"name":"Oracle","slug":"Oracle","permalink":"http://yoursite.com/tags/Oracle/"}]},{"title":"Spark访问数据库的几种方法","slug":"Spark访问数据库的几种方法","date":"2019-02-08T15:41:54.000Z","updated":"2019-03-27T07:51:06.199Z","comments":true,"path":"2019/02/08/Spark访问数据库的几种方法/","link":"","permalink":"http://yoursite.com/2019/02/08/Spark访问数据库的几种方法/","excerpt":"这篇文章涵盖了spark与常用关系型数据库交互的所有内容(oracle,sqlserver与mysql类似这里就不详细说明了)，这也是我项目中用到最常用的几种，应该可以帮助大家快速开发项目","text":"这篇文章涵盖了spark与常用关系型数据库交互的所有内容(oracle,sqlserver与mysql类似这里就不详细说明了)，这也是我项目中用到最常用的几种，应该可以帮助大家快速开发项目 Spark访问Hive数据库spark访问hive数据库有两种方式 第一种是原始的jdbc访问hive数据库 import java.sql.DriverManager object HiveJDBC { def main(args: Array[String]): Unit = { //1. 利用反射机制获取数据库驱动类 val driver = &quot;org.apache.hive.jdbc.HiveDriver&quot; Class.forName(driver) //2. 声明hive数据库的url,username,password val (url,username,password) = (&quot;jdbc:hive2://master:10000&quot;,&quot;spark&quot;,&quot;&quot;) //2.1 获取数据库连接对象 val connection = DriverManager.getConnection(url,username,password) //2.2 编写sql语句 val sql = &quot;select count(*) as count from sogou.sogou500w_ext&quot; //3. 执行sql语句 val statement = connection.prepareStatement(sql) //4. 处理结果集 val result = statement.executeQuery() //4. 循环打印结果 while(result.next()){ println(s&quot;${result.getString(&quot;mycount&quot;)}&quot;) } //5. 关闭连接 result.close() statement.close() connection.close() } } 第二种是通过spark自带的访问数据库的方法，在配置sparkSession时引入enableHiveSupport()支持hive的访问 import org.apache.log4j.{Level, Logger} import org.apache.spark.sql.SparkSession object ReadHive { //隐藏打印的配置信息 Logger.getLogger(&quot;org&quot;).setLevel(Level.ERROR) def main(args: Array[String]): Unit = { val spark = SparkSession.builder() .appName(&quot;Spark Hive Demo&quot;) .master(&quot;local&quot;) .enableHiveSupport()//支持hive，这个是关键，没有不行！ .getOrCreate() //spark.sparkContext.addJar(&quot;/home/ymliang/ReadHive.jar&quot;) //利用sparksql访问hive数据库，本质是编写sql语句 spark.sql(&quot;use sogou&quot;) //show-&gt;spark的action算子，打印结果，默认20行数据 spark.sql(&quot;select * from sogou.sogou500w_ext&quot;).show() spark.stop() } } Spark访问MySql数据库spark访问mysql数据库的三种方式 用jdbc方式访问mysql数据库 /** * 读取数据库中的数据 * 备注： * 1、将jdbc驱动拷贝到$SPARK_HOME/conf目录下，是最简单的做法； * 2、明白每一个参数的意思，一个参数不对整个结果出不来； * 3、从数据库从读大量的数据进行分析，不推荐；读取少量的数据是可以接受的，也是常见的做法。 */ object MySqlJDBC { def main(args: Array[String]): Unit = { val spark = SparkSession.builder() .master(&quot;local&quot;) .appName(s&quot;${this.getClass.getCanonicalName}&quot;) .getOrCreate() /** * 配置jdbc参数 * url : mysql的url * driver : mysql的数据库驱动类 * dbtable : 表名 * user : 用户名 * password : 密码 * load() : 加载 */ val jdbcDF = spark.read.format(&quot;jdbc&quot;) .option(&quot;url&quot;,&quot;jdbc:mysql://master:3306/spark&quot;) .option(&quot;driver&quot;,&quot;com.mysql.jdbc.Driver&quot;) .option(&quot;dbtable&quot;,&quot;student&quot;) .option(&quot;user&quot;,&quot;spark&quot;) .option(&quot;password&quot;,&quot;spark&quot;) .load() //将读出的数据库信息创建临时表，常用方法是createOrReplaceTempView(创建,如果存在则重新创建视图) jdbcDF.createTempView(&quot;tmp&quot;) //格式化打印表内容 spark.sql(&quot;select * from tmp&quot;).show() //打印列的Schema属性 jdbcDF.printSchema } } 创建一个数据库工具类(使用事务和批处理，提高性能) 首先为什么要使用批处理？ 批处理时：数据累积到一定数量，再一次性提交到数据库，减少了与数据库的交互次数，所以效率会大大提高 事务：事务指逻辑上的一组操作，组成这组操作的各个单元，要不全部成功，要不全部不成功，默认是关闭事务的。 import java.sql.{Connection, DriverManager, PreparedStatement, ResultSet} /** * 既用事务，也用批处理；（建议在处理大批量的数据时，同时使用批处理和事务） */ object MySqlUtils { /** * 获取一个数据库连接对象 * * @return Connection */ def getConnect(): Connection = { var conn: Connection = null var psts: PreparedStatement = null try { Class.forName(&quot;com.mysql.jdbc.Driver&quot;) conn = DriverManager.getConnection(&quot;jdbc:mysql://master:3306/spark&quot;, &quot;spark&quot;, &quot;spark&quot;) conn.setAutoCommit(false)//将自动提交关闭 } catch { case e:Exception =&gt; e.printStackTrace() } conn } /** * 关闭连接 con,psts,res Object */ //Connection conn, ResultSet rs, Statement st def closeRes(con: Connection, rs: ResultSet, psts: PreparedStatement): Unit = { if (con != null) { try { con.close() } catch { case e: Exception =&gt; e.printStackTrace() } } if (rs != null) { try { rs.close() } catch { case e: Exception =&gt; e.printStackTrace() } } if (psts != null) { try { psts.close() } catch { case e: Exception =&gt; e.printStackTrace() } } } } 具体使用这个工具类时，调用方法获取连接关闭链接即可，这里举一个之前做的小demo import java.sql.{Connection, PreparedStatement} import org.apache.log4j.{Level, Logger} import org.apache.spark.{SparkConf, SparkContext} object LogTest1 { //隐藏打印信息 Logger.getLogger(&quot;org&quot;).setLevel(Level.ERROR) def main(args: Array[String]): Unit = { val conf = new SparkConf() .setAppName(this.getClass.getCanonicalName) .setMaster(&quot;local&quot;) val sc = new SparkContext(conf) //读文件 val file = sc.textFile(&quot;hdfs://master:9000/sparkTest01.log&quot;) //处理文件 //数据文件大致内容如下: //192.168.88.1 - - [30/Jul/2017:12:53:43 +0800] &quot;GET /MyDemoWeb/ HTTP/1.1&quot; 200 259 //192.168.88.1 - - [30/Jul/2017:12:53:43 +0800] &quot;GET /MyDemoWeb/head.jsp HTTP/1.1&quot; 200 713 //192.168.88.1 - - [30/Jul/2017:12:53:43 +0800] &quot;GET /MyDemoWeb/body.jsp HTTP/1.1&quot; 200 240 //192.168.88.1 - - [30/Jul/2017:12:54:37 +0800] &quot;GET /MyDemoWeb/oracle.jsp HTTP/1.1&quot; 200 242 //192.168.88.1 - - [30/Jul/2017:12:54:38 +0800] &quot;GET /MyDemoWeb/hadoop.jsp HTTP/1.1&quot; 200 242 //192.168.88.1 - - [30/Jul/2017:12:54:38 +0800] &quot;GET /MyDemoWeb/java.jsp HTTP/1.1&quot; 200 240 val data = file.map(x =&gt; { //按&quot;切分 val line = x.split(&quot;\\&quot;&quot;) //切分开取第二个数据，再按/切分 val line2 = line(1).split(&quot;/&quot;) //取切分后的第三个个数据再按空格切分后取第一个数据即head.jsp //变为tuple形式(xx,1)返回 (line2(2).split(&quot;\\\\s+&quot;)(0), 1) }).collect.filter(_._1 != &quot;&quot;) //filter即取出数据为不为空的信息，起到过滤的作用 //做wordCount val data2 = data.groupBy(_._1).map(x =&gt; { //map后数据类型是 //x.1-&gt;String //x.2是Array(String,Int) //所以对x.2再遍历求和即得到出现的次数 (x._1, x._2.map(x =&gt; x._2).sum) }).toArray //返回的是数组形式 //JDBC访问mysql数据库，调用了MysqlUtils类获取连接和关闭链接 //并使用了批处理executeBatch(),addBatch() //相比于executeUpdate()效率更高 var conn: Connection = null var psts: PreparedStatement = null //jdbc try { //调用getConnect()获取链接 conn = MySqlUtils.getConnect() //执行sql语句 val psts = conn.prepareStatement(&quot;insert into test01 values(?,?)&quot;) for (i &lt;- 0 to data2.length - 1) { psts.setString(1, s&quot;${data2(i)._1}&quot;) psts.setInt(2, data2(i)._2) // 这样，更新10000条数据，就得访问数据库10000次,造成io的负载，数据量一大会明显察觉速度变慢 psts.executeUpdate() //psts.setString(1, s&quot;${data2(i)._1}&quot;) //psts.setInt(2, data2(i)._2) //psts.addBatch()//添加到同一个批处理中 } //批处理 executeBatch() 批量写入，降低IO，提高性能 //psts.executeBatch()//执行批处理 conn.commit(); //执行完后，手动提交事务 conn.setAutoCommit(true); //在把自动提交打开 } catch { case e: Exception =&gt; e.printStackTrace() } finally { //调用closeRes()关闭链接 MySqlUtils.closeRes(conn, null, psts) } } } 用spark方式链接mysql，前提是必须创建dataFrame(df)，这里以hive写入mysql为例，将hive同mysql整合起来，也是平常非常常用的一种技术 import java.util.Properties import org.apache.log4j.{Level, Logger} import org.apache.spark.sql.{SaveMode, SparkSession} /** spark将最后一条sql语句的查询结果保存到mysql数据库中*/object HiveToMySql { Logger.getLogger(“org”).setLevel(Level.ERROR) def main(args: Array[String]): Unit = { val spark = SparkSession.builder() .appName(&quot;Spark Hive Demo&quot;) .master(&quot;spark://master:7077&quot;) .enableHiveSupport() //支持hive，这个是关键，没有不行！ .getOrCreate() //读取hive数据库内容 spark.sql(&quot;use sogou&quot;) //获取dataFrame val df = spark.sql(&quot;select count(*) as mycount from sogou.sogou500w_ext&quot;) //创建properties对象，注入mysql配置 val prop = new Properties() prop.put(&quot;user&quot;, &quot;spark&quot;) prop.put(&quot;password&quot;, &quot;spark&quot;) prop.put(&quot;driver&quot;, &quot;com.mysql.jdbc.Driver&quot;) //写入数据时字段名要对应上，顺序可以不与实际的表对应也就是mysql与hive的表列字段要一致 df.write .mode(SaveMode.Overwrite) .jdbc(&quot;jdbc:mysql://master:3306/spark&quot;, &quot;spark.bb&quot;, prop) //写入mysql时可以配置mode(插入)，overwrite(覆盖)，append(追加)，ignore(忽略)，error(默认表存在报错)常用是mode和overwrite //resultDF.write.mode(SaveMode.Overwrite).jdbc(&quot;jdbc:mysql://192.168.200.150:3306/spark&quot;,&quot;student&quot;,prop) }} 以上涵盖了spark与常用关系型数据库交互的所有内容(oracle,sqlserver与mysql类似这里就不详细说明了)，希望对大家有所帮助，这也是我项目中用到最常用的几种，应该可以帮助大家快速开发项目","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/数据库/"}],"tags":[{"name":"sparkSql","slug":"sparkSql","permalink":"http://yoursite.com/tags/sparkSql/"}]},{"title":"Hbase之HMaster启动后经常掉的解决方案","slug":"Hbase之HMaster启动后经常掉的解决方案","date":"2019-02-08T11:08:03.000Z","updated":"2019-02-08T11:19:11.728Z","comments":true,"path":"2019/02/08/Hbase之HMaster启动后经常掉的解决方案/","link":"","permalink":"http://yoursite.com/2019/02/08/Hbase之HMaster启动后经常掉的解决方案/","excerpt":"","text":"在Hbase-shell中执行命令list之后报了一串错误…如何解决？ SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in [jar:file:/home/xdl/hbase-0.98.9-hadoop2/lib/slf4j-log4j12-1.6.4.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: Found binding in [jar:file:/home/xdl/hadoop-2.5.2/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. 2018-10-19 05:31:49,934 WARN [main] util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable --以上是因为Hadoop的依赖包与hbase的依赖包中的jar包相同，不影响启动关键在于下面的异常： 服务起不来，看一下具体报错 ERROR: org.apache.hadoop.hbase.ipc.ServerNotRunningYetException: Server is not running yet Server is not running yet 服务起不来 --第一个想到的方法是关闭hbase重新启动 stop-hbase.sh start-hbase.sh 果然是没卵用的 网上还有说是让关闭hadoop的安全模式 查看hadoop的安全模式状态： hadoop dfsadmin -safemode get Safe mode is OFF OFF：表示安全模式关闭状态，如果是ON则可以用 hadoop dfsadmin -safemode leave 命令来关闭安全模式 果然第二个方法也没用，那换第三个方法，网上还有说是hbase的依赖包与hadoop的依赖包重复让删除hbase中的重复的依赖包： slf4j-log4j12-1.6.4.jar slf4j-api-1.6.4.jar 果然删了以后还是没卵用 那么list后出现的ERROR最有用的是 ZOOKEEPER的问题： ERROR:can&#39;t get master address from ZooKeeper; znode data == null 那我们就从这里入手，从zookeeper的data路径： (三个节点都删)(这里以三个节点(1主2从)为例) /home/zkpk/zookeeper-3.4.5/data 下的 myid zookeeper_server.pid 除了myid以外的其他文件都删掉 然后重启zookeeper ： zkServer.sh restart (三个节点都重启zookeeper) 这时候应该不会报:ERROR:can&#39;t get master address from ZooKeeper; znode data == null这个错误了 而且HMaster也应该不会掉了(其实我的HMaster在我每次进入hbase shell 后 list 后就掉了…) 但当你再进入 hbase shell 再 list时还可能报：ERROR: org.apache.hadoop.hbase.ipc.ServerNotRunningYetException: Server is not running yet 这个错误 解决方法就是：格式化namenode (我就是这样好的)，如何格式化？ hadoop namenode -format 然后启动Hadoop集群(start-all.sh)，zookeeper(前面重启过zookeeper，这里就不用重启了)，hbase(start-hbase.sh{这里说一嘴，关闭hbase时从节点上的HRegionServer关不掉可以直接KILL -9 掉它}) 再进入hbase shell中去测试吧，应该没问题了！！ 总结一下，出了错不要总是在Hbase上找问题，关键要去集群环境，zookeeper环境上寻找，最坏的做法就是格式化集群，这样会导致hdfs上的文件清空，还有就是zookeeper文件夹下的data文件夹里面的内容除了配置的myid不需要删以外，其他的都删掉，你下次再启动zookeeper后会自动生成所以不太影响，但问题往往会出在这里，多试一些方法，解决问题最重要","categories":[],"tags":[{"name":"Hbase","slug":"Hbase","permalink":"http://yoursite.com/tags/Hbase/"}]},{"title":"结构化数据、半结构化数据和非结构化数据","slug":"结构化数据、半结构化数据和非结构化数据","date":"2019-02-06T16:03:51.000Z","updated":"2019-02-18T09:46:17.493Z","comments":true,"path":"2019/02/07/结构化数据、半结构化数据和非结构化数据/","link":"","permalink":"http://yoursite.com/2019/02/07/结构化数据、半结构化数据和非结构化数据/","excerpt":"","text":"结构化数据、半结构化数据和非结构化数据什么是结构化数据、半结构化数据和非结构化数据？ (1)结构化数据结构化的数据是指可以使用关系型数据库表示和存储，表现为二维形式的数据。一般特点是：数据以行为单位，一行数据表示一个实体的信息，每一行数据的属性是相同的。举一个例子： id name age gender 1 lyg 12 male 2 liangym 13 female 3 liang 18 male 所以，结构化的数据的存储和排列是很有规律的，这对查询和修改等操作很有帮助。但是，显然，它的扩展性不好（比如，我希望增加一个字段，怎么办？）。 (2)半结构化数据 半结构化数据是结构化数据的一种形式，它并不符合关系型数据库或其他数据表的形式关联起来的数据模型结构，但包含相关标记，用来分隔语义元素以及对记录和字段进行分层。因此，它也被称为自描述的结构。 半结构化数据，属于同一类实体可以有不同的属性，即使他们被组合在一起，这些属性的顺序并不重要。 常见的半结构数据有XML和JSON，对于对于两个XML文件，第一个可能有 &lt;person&gt; &lt;name&gt;A&lt;/name&gt; &lt;age&gt;13&lt;/age&gt; &lt;gender&gt;female&lt;/gender&gt; &lt;/person&gt; 第二个可能为： &lt;person&gt; &lt;name&gt;B&lt;/name&gt; &lt;gender&gt;male&lt;/gender&gt; &lt;/person&gt; 从上面的例子中，属性的顺序是不重要的，不同的半结构化数据的属性的个数是不一定一样的。有些人说半结构化数据是以树或者图的数据结构存储的数据，怎么理解呢？ 上面的例子中，&lt;person&gt;标签是树的根节点，&lt;name&gt;和&lt;gender&gt;标签是子节点。通过这样的数据格式，可以自由地表达很多有用的信息，包括自我描述信息（元数据）。 所以，半结构化数据的扩展性是很好的。 (3)非结构化数据 顾名思义，就是没有固定结构的数据。各种文档、图片、视频/音频等都属于非结构化数据。对于这类数据，我们一般直接整体进行存储，而且一般存储为二进制的数据格式。","categories":[{"name":"面经","slug":"面经","permalink":"http://yoursite.com/categories/面经/"}],"tags":[{"name":"面试题","slug":"面试题","permalink":"http://yoursite.com/tags/面试题/"}]},{"title":"如何实现单点登录验证功能","slug":"如何实现单点登录验证功能","date":"2019-02-02T18:36:42.000Z","updated":"2019-02-05T17:45:11.867Z","comments":true,"path":"2019/02/03/如何实现单点登录验证功能/","link":"","permalink":"http://yoursite.com/2019/02/03/如何实现单点登录验证功能/","excerpt":"","text":"单点登录(SSO =&gt; Single Sign On) 单点登录SSO是指Single Sign On 从一个系统登录，其他系统免登录，在一个系统用户增多，架构完善之后为了方便运营人员登录多个系统而不需要每次都输入用户名密码，所以实现单点登录显得尤为重要，这里分享一下我在项目中如何用单点登录实现用户登录一次即可访问其他相互信任的系统 首先我们的认证系统是独立的一个模块，即我们在用户登录后，会调用认证系统的方法来生成用户唯一的session并进行保存，认证系统返回给客户端一个唯一的登录凭证，用这个登录凭证即可直接访问其他的子系统不用再进行登录了 在进行登录认证返回登录凭证后，我们还需要配置一个拦截器，用于拦截服务的请求，即我们需要哪些请求要验证这个登录凭证才能访问我们的其他服务，这样写可能不是很清楚，举个栗子 用户A登录网站，以post请求访问/xxx/login，再登录的同时去注册验证系统，返回一个登录状态(成功登录or失败)并附带一个登录验证后的凭证，这个凭证代表A可以访问其他的子服务 当用户A访问B服务时拦截器起作用，拦截器控制如果想访问B服务，需要验证这个凭证是否生效，如果生效则放行，这就是单点登录如何通过配置拦截器控制访问的实现流程 public class CheckInterceptor implements HandlerInterceptor { //发请求 restTemplate //专门调rest服务的对象 @Autowired private RestTemplate restTemplate; public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { //截取请求中的userId,ticket //ticket为凭证,这里凭证已经通过登录系统保存至浏览器,我们直接取即可 String userId = request.getParameter(&quot;userId&quot;); String ticket = request.getParameter(&quot;ticket&quot;); // 将userId,ticket发给/user/ticket检测 //返回结果是ReturnResult对象 if(userId!=null&amp;&amp;ticket!=null){ //这里是拦截器需要访问这个请求,并设置参数进行验证 String url = &quot;http://localhost:7001/user/ticket&quot;; MultiValueMap&lt;String, Object&gt; params = new LinkedMultiValueMap&lt;&gt;(); params.set(&quot;userId&quot;, userId); params.set(&quot;ticket&quot;, ticket); ReturnResult result = restTemplate.postForObject(url, params, ReturnResult.class); //返回结果的判断 if(result.getStatus()==yixueConstant.SUCCESS){ //校验正确,放行 return true; } } //校验失败,拦截请求,给用户返回一个json结果 response.setContentType(&quot;text/html;Charset=UTF-8&quot;); PrintWriter out = response.getWriter(); out.print(&quot;{\\&quot;status\\&quot;:-2,\\&quot;msg\\&quot;:\\&quot;令牌凭证无效\\&quot;}&quot;); out.flush(); return false; } } 有了拦截器，我们还要把拦截器加入配置，使其生效 @Component public class CheckInterceptorConfiguration implements WebMvcConfigurer { @Autowired private CheckInterceptor checkInterceptor; public void addInterceptors(InterceptorRegistry registry) { //添加要拦截的请求地址url，在我们访问视频模块时，需要先拦截请求，在拦截器中验证，如果返回结果为true则放行 String[] urls = {&quot;/course/chapter&quot;}; registry.addInterceptor(checkInterceptor).addPathPatterns(urls); } } 具体的完整流程（凭证的生成及存储）请参考我的另外一篇文章 这里我分享另外一篇技术博客给大家方便大家理解单点登录","categories":[],"tags":[]},{"title":"浅谈项目中用到redis的地方","slug":"浅谈项目中用到redis的地方","date":"2019-02-02T18:32:38.000Z","updated":"2019-02-04T17:14:42.175Z","comments":true,"path":"2019/02/03/浅谈项目中用到redis的地方/","link":"","permalink":"http://yoursite.com/2019/02/03/浅谈项目中用到redis的地方/","excerpt":"","text":"之前和几个朋友在开发网站时用到了redis数据库，这里分享干货给大家，废话不多说直接上代码 我们用到redis的地方是在验证登录时的一个小demo，思路是每个用户登录到网站时给这个用互提供一个唯一的校验码ticket，之后存到redis中在后续访问网站相关内容时要取出ticket进行验证方可继续访问 生成ticket entity层 Ticket.java private Integer userId;//用户id private String userName;//用户名 private String token;//ticket编号 private long createTime;//创建时间 private long expire;//有效期 public String toString() { // token+createTime+expire+userId+userName String str = token + &quot;-&quot; + createTime + &quot;-&quot; + expire + &quot;-&quot; + userId; //利用Base64算法处理 String base64Str = Base64Utils.encodeToString(str.getBytes()); return base64Str; } ...get,set方法自己生成吧 这里用到了Base64算法封装ticket，便于验证时的比较 service层 TicketManager.java//管理ticket的TickManager类，有创建ticket的方法和检验ticket的方法 @Component public class TicketManager { @Autowired private RedisTemplate&lt;Object, Object&gt; redis; public Ticket create(User user, int hour) { Ticket ticket = new Ticket(); ticket.setUserId(user.getId()); ticket.setUserName(user.getName()); ticket.setCreateTime(System.currentTimeMillis()); ticket.setExpire(hour * 3600 * 1000);// 有效时长 ticket.setToken(UUID.randomUUID().toString()); // 将ticket存入redis,方便将来验证 // redis.opsForValue().set(&quot;ticket_&quot;+user.getId(), t); redis.opsForHash().put(&quot;tickers&quot;, user.getId(), ticket); // 测试取userId Ticket t = (Ticket) redis.opsForHash().get(&quot;tickers&quot;, user.getId()); System.out.println(&quot;从redis中读取=&gt;:&quot;+t); return ticket; } public int checkTicket(int userId, String ticket) { Ticket t = (Ticket) redis.opsForHash().get(&quot;tickers&quot;, userId); System.out.println(&quot;|--------------------------------------------------------------------------------------------------|&quot;); System.out.println(&quot;|从redis中读取=&gt;:&quot;+t.toString()+&quot;|&quot;); System.out.println(&quot;|--------------------------------------------------------------------------------------------------|&quot;); System.out.println(&quot;|从request带来=&gt;:&quot;+ticket.toString()+&quot;|&quot;); System.out.println(&quot;|--------------------------------------------------------------------------------------------------|&quot;); System.out.println(&quot;|&quot;+t.toString().equals(ticket)+&quot;|&quot;); System.out.println(&quot;|--------------------------------------------------------------------------------------------------|&quot;); // 检测ticket是否匹配 if (t != null &amp;&amp; t.toString().equals(ticket)) { // 匹配成功 long currentTime = System.currentTimeMillis(); long totalTime = t.getCreateTime() + t.getExpire(); // 检测是否失效 if (currentTime &lt; totalTime) { // 在有效期内 0 return yixueConstant.SUCCESS; } else { // 过期 1 return yixueConstant.ERROR1; } } // 不匹配 -1 return yixueConstant.ERROR; } } 这里用到了Java访问Redis的方法因为网站是用SpringBoot开发的，boot也提供了访问redis的包 spring-boot-starter-data-redis 这里我提供boot访问redis的方法： 需要添加SpringData-redis包，它提供一个RedisTemplate对象使用。 引入spring-boot-starter-data-redis工具包(Maven项目中的pom.xml中添加) &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 在application.properties配置redis连接参数(resources文件夹下的application.properties配置文件) spring.redis.host=localhost spring.redis.port=6379 注入使用RedisTemplate对象(这是测试类，测试能否存储到redis中和能否取出) @RunWith(SpringRunner.class) @SpringBootTest(classes={MyBootApplication.class}) public class TestRedisTemplate { @Autowired private RedisTemplate&lt;Object, Object&gt; redis; @Test public void test1(){ redis.opsForValue().set(&quot;msg1&quot;, &quot;redis&quot;); String value = (String)redis.opsForValue().get(&quot;msg1&quot;); System.out.println(value); } } 接着我们在service接口添加方法，这个方法是检验ticket是否合法和是否超时 /** * 检查凭证是否合法或超时功能的实现 * @param userId 用户唯一ID * @param ticket 用户登录后唯一凭证 * @return */ public ReturnResult checkTicket(int userId,String ticket); 在实现类serviceImp中注入ticketManager对象 @Autowired private TicketManager ticketManager; 在实现类serviceImp中有用户登录的方法和检验ticket的方法，我们需要在用户登录方法中调用create方法来实现用户每次登录时都创建一个唯一的ticket作为凭证，用户如果要想访问网站其他内容必须要经过ticket的校验，这一点我们后面用拦截器来实现 /** * 登录功能实现 */ @Override public ReturnResult checkUser(String name, String password) { ReturnResult result = new ReturnResult(); // 检验参数是否合法 if (StringUtils.isEmpty(name) || StringUtils.isEmpty(password)) { // 用yixueConstant常量类替换 // (yixueConstant.ERROR -&gt; 参数错误 // yixueConstant.PARAM_ERROR-&gt;&quot;参数不合法&quot;） result.setStatus(yixueConstant.ERROR); result.setMsg(yixueConstant.PARAM_ERROR); return result; } // 检查账户是否存在 User user = userDao.selectByName(name); if (user == null) { // 不存在 result.setStatus(yixueConstant.ERROR1); result.setMsg(yixueConstant.LOGIN_NAME_ERROR); return result; } // 检验密码是否正确 String salt = user.getSalt(); String md5Password = PasswordUtil.md5(password + salt); if (!user.getPassword().equals(md5Password)) { result.setStatus(yixueConstant.ERROR2); result.setMsg(yixueConstant.LOGIN_PASSWORD_ERROR); return result; } // 账户和密码都正确 result.setStatus(yixueConstant.SUCCESS); result.setMsg(yixueConstant.LOGIN_SUCCESS); //---关键代码---- // 若登录成功即(账户和密码都正确),则创建一个ticket返回,有效期2小时 Ticket ticket = ticketManager.create(user,2); Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put(&quot;userId&quot;, ticket.getUserId()); //这里我们把ticket的toString方法的返回值放进map中了，也就是base64算法封装后的ticket map.put(&quot;ticket&quot;, ticket.toString()); //---关键代码---- result.setData(map); //返回的result是返回给浏览器，浏览器把ticket保存下来，继续后续的业务校验工作 return result; } /** * 检查凭证是否合法或超时功能 * */ @Override public ReturnResult checkTicket(int userId, String ticket) { ReturnResult result = new ReturnResult(); int num = ticketManager.checkTicket(userId, ticket); if (num == yixueConstant.SUCCESS){ // 凭证在有效期内 result.setStatus(num); result.setMsg(yixueConstant.TICKETS_SUCCESS); return result; }else if(num == yixueConstant.ERROR1){ //凭证过时 result.setStatus(num); result.setMsg(yixueConstant.TICKETS_ERROR_OVERTIME); return result; } //凭证不匹配 result.setStatus(num); result.setMsg(yixueConstant.TICKETS_ERROR); return result; } returnResult 对象是 ReturnResult类的对象，是为了给浏览器返回一个状态码，状态信息，和数据内容，我们用ajax来取出传来的data信息进行前端的展示 private int status; private String msg; private Object data; - Controller层中注入service并添加方法，这个方法也就是我们后面如果有请求被拦截，都会调用这个方法来进行校验的 @Autowired private UserService userService; @PostMapping(&quot;/user/ticket&quot;) public ReturnResult ticket( @RequestParam(name = &quot;userId&quot;, required = false) int userId, @RequestParam(name = &quot;ticket&quot;, required = false) String ticket) { ReturnResult result = userService.checkTicket(userId,ticket); return result; } - 之后我们如果想拦截用户访问其他模块的请求，就在相应的模块中添加拦截器，这里以访问视频模块为例，在视频模块中添加intercepter拦截器 public class CheckInterceptor implements HandlerInterceptor { //发请求 restTemplate //专门调rest服务的对象 @Autowired private RestTemplate restTemplate; public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { //截取请求中的userId,ticket String userId = request.getParameter(&quot;userId&quot;); String ticket = request.getParameter(&quot;ticket&quot;); // 将userId,ticket发给/user/ticket检测 //返回结果是ReturnResult对象 if(userId!=null&amp;&amp;ticket!=null){ String url = &quot;http://localhost:7001/user/ticket&quot;; MultiValueMap&lt;String, Object&gt; params = new LinkedMultiValueMap&lt;&gt;(); params.set(&quot;userId&quot;, userId); params.set(&quot;ticket&quot;, ticket); ReturnResult result = restTemplate.postForObject(url, params, ReturnResult.class); if(result.getStatus()==yixueConstant.SUCCESS){ //校验正确,放行 return true; } } //校验失败,拦截请求,给用户返回一个json结果 response.setContentType(&quot;text/html;Charset=UTF-8&quot;); PrintWriter out = response.getWriter(); out.print(&quot;{\\&quot;status\\&quot;:-2,\\&quot;msg\\&quot;:\\&quot;令牌凭证无效\\&quot;}&quot;); out.flush(); return false; } } - 有了拦截器，我们还要把拦截器加入配置，使其生效 @Component public class CheckInterceptorConfiguration implements WebMvcConfigurer { @Autowired private CheckInterceptor checkInterceptor; public void addInterceptors(InterceptorRegistry registry) { //添加要拦截的请求地址url，在我们访问视频模块时，需要先拦截请求，在拦截器中验证，如果返回结果为true则放行 String[] urls = {&quot;/course/chapter&quot;}; registry.addInterceptor(checkInterceptor).addPathPatterns(urls); } } 到这里大致就结束了，其实用到redis的地方就是springBoot访问redis的包，和访问redis的几行代码，这里稍微总结一下： @Autowired private RedisTemplate&lt;Object, Object&gt; redis; 注入并使用RedisTemplate对象 - 存到redis中 `redis.opsForHash().put(&quot;tickers&quot;, user.getId(), ticket);` - 取出 `redis.opsForHash().get(&quot;tickers&quot;, user.getId());` 也就是我们在用户登录是要create一个ticket存入redis中，然后在checkTicket时我们从redis取出ticket做校验，如果相同再去校验是否超时达到单点登录的功能 这里附一个redis的教程供大家参考","categories":[],"tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"SparkSQL分析","slug":"SparkSQL面试题","date":"2019-02-02T16:05:30.000Z","updated":"2019-05-15T02:37:47.714Z","comments":true,"path":"2019/02/03/SparkSQL面试题/","link":"","permalink":"http://yoursite.com/2019/02/03/SparkSQL面试题/","excerpt":"SparkSql作为Spark的结构化数据处理模块，提供了非常强大的API，让分析人员用一次，就会为之倾倒，为之着迷，为之至死不渝。在内部，SparkSQL使用额外结构信息来执行额外的优化。在外部，可以使用SQL和DataSet 的API与之交互。","text":"SparkSql作为Spark的结构化数据处理模块，提供了非常强大的API，让分析人员用一次，就会为之倾倒，为之着迷，为之至死不渝。在内部，SparkSQL使用额外结构信息来执行额外的优化。在外部，可以使用SQL和DataSet 的API与之交互。 简单排名函数的使用昨天和今天都登录了的用户id昨天登录了但是今天没有登录的用户id --ds1 中的数据代表昨天登录的用户id --ds2 中的数据代表今天登录的用户id --准备数据 val ds1 = spark.range(0,20) val ds2 = spark.range(10,30) ds1.createOrReplaceTempView(&quot;t1&quot;) ds2.createOrReplaceTempView(&quot;t2&quot;) --求： --1. 昨天和今天都登录了的用户id --2. 昨天登录了但是今天没有登录的用户id --1. 这里用的是简单的表连接求交集,比较简单,如果看不懂可以把sql语句拆开看效果 sql(&quot;&quot;&quot; select id1 from( select t1.id as id1,if(isnull(t2.id),&apos;未出现&apos;,t2.id) as id2 from t1 left join t2 on t1.id = t2.id )where id1 = id2 &quot;&quot;&quot;).show +---+ |id1| +---+ | 10| | 11| | 12| | 13| | 14| | 15| | 16| | 17| | 18| | 19| +---+ --2. 在1的基础上稍微变一下,因为求第一天出现第二天没出线时只需要满足第二天的=未出现即可, sql(&quot;&quot;&quot; select id1 from( select t1.id as id1,if(isnull(t2.id),&apos;未出现&apos;,t2.id) as id2 from t1 left join t2 on t1.id = t2.id )where id2 = &apos;未出现&apos; &quot;&quot;&quot;).show +---+ |id1| +---+ | 0| | 1| | 2| | 3| | 4| | 5| | 6| | 7| | 8| | 9| +---+ --3. 如果是求第二天出现第一天未出现的话t1 t2换个位置就好了 sql(&quot;&quot;&quot; select id2 from( select if(isnull(t1.id),&apos;未出现&apos;,t1.id) as id1,t2.id as id2 from t1 right join t2 on t1.id = t2.id )where id1 = &apos;未出现&apos; &quot;&quot;&quot;).show +---+ |id2| +---+ | 20| | 21| | 22| | 23| | 24| | 25| | 26| | 27| | 28| | 29| +---+ 这里如果对isnull函数不理解的话可以参考我写的,逻辑就是如果(if)字段为空isnull(字段),就替换为’未出现’ 否则还是原有字段,好像还有其他写法,大家自行百度吧,达到需求就可以了 if(isnull(t1.id),&#39;未出现&#39;,t1.id) 每个部门工资最高的前3名--这个数据表头是csv文件格式便于sparksql进行自动类型匹配 EMPNO,ENAME,JOB,MGR,HIREDATE,SAL,COMM,DEPTNO 7369,SMITH,CLERK,7902,2001-01-02 22:12:13,800,,20 7499,ALLEN,SALESMAN,7698,2002-01-02 22:12:13,1600,300,30 7521,WARD,SALESMAN,7698,2003-01-02 22:12:13,1250,500,30 7566,JONES,MANAGER,7839,2004-01-02 22:12:13,2975,,20 7654,MARTIN,SALESMAN,7698,2005-01-02 22:12:13,1250,1400,30 7698,BLAKE,MANAGER,7839,2005-04-02 22:12:13,2850,,30 7782,CLARK,MANAGER,7839,2006-03-02 22:12:13,2450,,10 7788,SCOTT,ANALYST,7566,2007-03-02 22:12:13,3000,,20 7839,KING,PRESIDENT,,2006-03-02 22:12:13,5000,,10 7844,TURNER,SALESMAN,7698,2009-07-02 22:12:13,1500,0,30 7876,ADAMS,CLERK,7788,2010-05-02 22:12:13,1100,,20 7900,JAMES,CLERK,7698,2011-06-02 22:12:13,950,,30 7902,FORD,ANALYST,7566,2011-07-02 22:12:13,3000,,20 7934,MILLER,CLERK,7782,2012-11-02 22:12:13,1300,,10 --最后达到的要求 +------+----+----+----+ |deptno|sal1|sal2|sal3| +------+----+----+----+ | 10|5000|2450|1300| | 20|3000|3000|2975| | 30|2850|1600|1500| +------+----+----+----+ --(&quot;header&quot;, true)表示如果这个是数据本身是有列名的就按第一行的列名去建表,列名就是这个文件的第一行的每个单词 --(&quot;inferschema&quot;, true)表示自动类型推断,我们可以用df.printSchema来看spark给我们匹配的类型是什么 --.csv(&quot;hdfs://master:9000/data/a.log&quot;) val df = spark.read.option(&quot;header&quot;,true).option(&quot;inferschema&quot;,true).csv(&quot;hdfs://master:9000/data/a.log&quot;) --建临时表 df.createOrReplaceTempView(&quot;temp&quot;) row_number() over() –&gt; 排名函数的用法 --这里我都是分布来算的,便于自己理清思路,同时用多层子查询时这样写思路会很清晰 sql(&quot;&quot;&quot; select deptno,sal,row_number() over(partition by deptno order by sal desc) as salnum from temp having salnum &lt; 4 &quot;&quot;&quot;).show +------+----+------+ |deptno| sal|salnum| +------+----+------+ | 20| 800| 1| | 20|1100| 2| | 20|2975| 3| | 10|1300| 1| | 10|2450| 2| | 10|5000| 3| | 30| 950| 1| | 30|1250| 2| | 30|1250| 3| +------+----+------+ collect_list(sal) 列转行 配合group by来使用 sql(&quot;&quot;&quot; select deptno,collect_list(sal) as sal from( select deptno,sal,row_number() over(partition by deptno order by sal desc) as salnum from temp having salnum &lt; 4) group by deptno order by deptno &quot;&quot;&quot;).show +------+------------------+ |deptno| sal| +------+------------------+ | 10|[5000, 2450, 1300]| | 20|[3000, 3000, 2975]| | 30|[2850, 1600, 1500]| +------+------------------+ --sal[0],sal[1],sal[2]实质上类似于数组的取值,可以把sal看作是数组,这一步与求结果无关只是展示一下怎么取值 sql(&quot;&quot;&quot; select deptno,sal[0],sal[1],sal[2] from( select deptno,collect_list(sal) as sal from( select deptno,sal,row_number() over(partition by deptno order by sal desc) as salnum from temp having salnum &lt; 4) group by deptno) order by deptno &quot;&quot;&quot;).show +------+------+------+------+ |deptno|sal[0]|sal[1]|sal[2]| +------+------+------+------+ | 10| 5000| 2450| 1300| | 20| 3000| 3000| 2975| | 30| 2850| 1600| 1500| +------+------+------+------+ --求每个部门的最高工资 --first_value(sal) sal中的第一个值 sql(&quot;&quot;&quot; select distinct deptno,first from ( select deptno,sal,first_value(sal) over(partition by deptno order by sal desc) as first from temp )order by deptno &quot;&quot;&quot;).show +------+-----+ |deptno|first| +------+-----+ | 10| 5000| | 20| 3000| | 30| 2850| +------+-----+ --求每个部门的最低工资 --求的时候order by 别加desc就好了 sql(&quot;&quot;&quot; select distinct deptno,first from ( select deptno,sal,first_value(sal) over(partition by deptno order by sal) as first from temp )order by deptno &quot;&quot;&quot;).show +------+-----+ |deptno|first| +------+-----+ | 10| 1300| | 20| 800| | 30| 950| +------+-----+ 求NBA球队连冠记录是在哪年到哪年--这个数据与上一题的数据格式一样的 team,y 活塞,1990 公牛,1991 公牛,1992 公牛,1993 火箭,1994 火箭,1995 公牛,1996 公牛,1997 公牛,1998 马刺,1999 湖人,2000 湖人,2001 湖人,2002 马刺,2003 活塞,2004 马刺,2005 热火,2006 马刺,2007 凯尔特人,2008 湖人,2009 湖人,2010 --最终数据 +----+------+------+ |team|min(y)|max(y)| +----+------+------+ |公牛| 1991| 1993| |火箭| 1994| 1995| |公牛| 1996| 1998| |湖人| 2000| 2002| |湖人| 2009| 2010| +----+------+------+ --做这个题的时候我们要清除题的要求,是求连冠的球队,连冠指的是中间不能有其他球队夺冠,必须&gt;=2才可以,而且年份要挨着 val df2 = spark.read. option(&quot;header&quot;, true). option(&quot;inferschema&quot;, true). csv(&quot;hdfs://master:9000/data/team.log&quot;) df2.createOrReplaceTempView(&quot;t2&quot;) --这里我也是分步骤来求,便于理清思路 --这里用了y-row_number(),为什么这么用,在于如果是连冠,row_number()加排名函数,按球队名称做partition by,用夺冠年份减去row_number得到的数就是相同的,这里比较难理解,如果搞懂这里题也就做完了 -这里的a是分组排名的序号 aa也就是 y-a的值 sql(&quot;&quot;&quot; select team,y,a,aa from ( select team,y,row_number() over(partition by team order by y)as a,y-row_number() over(partition by team order by y)as aa from t2) &quot;&quot;&quot;).show +----+----+---+----+ |team| y| a| aa| +----+----+---+----+ | 热火|2006| 1|2005| | 活塞|1990| 1|1989| | 活塞|2004| 2|2002| | 火箭|1994| 1|1993| | 火箭|1995| 2|1993| |凯尔特人|2008| 1|2007| | 湖人|2000| 1|1999| | 湖人|2001| 2|1999| | 湖人|2002| 3|1999| | 湖人|2009| 4|2005| | 湖人|2010| 5|2005| | 公牛|1991| 1|1990| | 公牛|1992| 2|1990| | 公牛|1993| 3|1990| | 公牛|1996| 4|1992| | 公牛|1997| 5|1992| | 公牛|1998| 6|1992| | 马刺|1999| 1|1998| | 马刺|2003| 2|2001| | 马刺|2005| 3|2002| +----+----+---+----+ --只有重复出现的连续team,之后减去row_number()后才会有相同的aa列字段 此相同的aa字段代表在相同的字段下，球队是处于连冠状态 --按球队和aa分组，如果不按aa分组 那么公牛的连冠记录中间隔的几年也会被算进去,这样处理完的数据就不算是连冠的数据 sql(&quot;&quot;&quot; select team,min(y) miny,max(y) maxy from( select team,y,y-row_number() over(partition by team order by y)as aa from t2) group by team,aa &quot;&quot;&quot;).show +----+----+----+ |team|miny|maxy| +----+----+----+ | 热火|2006|2006| | 活塞|1990|1990| | 活塞|2004|2004| | 火箭|1994|1995| |凯尔特人|2008|2008| | 湖人|2000|2002| | 湖人|2009|2010| | 公牛|1991|1993| | 公牛|1996|1998| | 马刺|1999|1999| | 马刺|2003|2003| | 马刺|2005|2005| | 马刺|2007|2007| +----+----+----+ --判断最小值是否小于最大值，目的是为了排除单独一年获得冠军的球队即最小值=最大值，取出连冠过的球队 --最终要尽可能的精简代码,这样会给面试官一个好印象(代码洁癖什么的???),也会显得自己sql写的很好 sql(&quot;&quot;&quot; select team,min(y) miny,max(y) maxy from( select team,y,y-row_number() over(partition by team order by y)as aa from t2) group by team,aa having min(y)&lt;max(y) order by miny &quot;&quot;&quot;).show -- 大功告成 +----+----+----+ |team|miny|maxy| +----+----+----+ | 公牛|1991|1993| | 火箭|1994|1995| | 公牛|1996|1998| | 湖人|2000|2002| | 湖人|2009|2010| +----+----+----+ 好啦大功告成！！！，有问题可以联系我我们一起交流616581760 微信QQ同号","categories":[{"name":"sparkSql","slug":"sparkSql","permalink":"http://yoursite.com/categories/sparkSql/"}],"tags":[{"name":"sparkSql","slug":"sparkSql","permalink":"http://yoursite.com/tags/sparkSql/"}]},{"title":"SQL求K线波峰波谷","slug":"SQL求K线波峰波谷","date":"2019-02-02T14:52:06.000Z","updated":"2019-05-06T06:56:10.242Z","comments":true,"path":"2019/02/02/SQL求K线波峰波谷/","link":"","permalink":"http://yoursite.com/2019/02/02/SQL求K线波峰波谷/","excerpt":"","text":"利用SparkSQL求股市K线波峰波谷 前几天朋友问了一道题我觉得还不错拿出来分享一下，如何求股市K线的波峰波谷，这里我简单分析一下我的思路，首先K线是一系列无序的数字组成的，根据每个数字与其相邻的数字之间的大小关系我们就能很轻易的求出波峰波谷，下面开始分析 首先数据我们用scala生成 代码在spark-shell中写，如果在工作中大家遇到类似问题读文件取数据即可，这里简化用spark-shell代替 val random = new scala.util.Random(); val arr = (0 to 50).map(x=&gt;{random.nextInt(100)}).zipWithIndex zipWithIndex是在生成得随机数后在跟一个从0开始的索引组成一个个类似tuple形式的Vector容器 arr //看一下打印出的类型 scala&gt; res10: scala.collection.immutable.IndexedSeq[(Int, Int)] = Vector((85,0), (54,1), (69,2)...... 将其变为数组 val arr2 = arr.toArray 转化为RDD val rdd = sc.makeRDD(arr2) rdd转为DF(即DataFrame) val df = rdd.toDF(&quot;data&quot;,&quot;id&quot;) df.show +----+---+ |data| id| +----+---+ | 85| 0| | 54| 1| | 69| 2| | 7| 3| | 39| 4| | 40| 5| | 43| 6| | 50| 7| | 59| 8| | 28| 9| | 98| 10| | 70| 11| | 87| 12| | 52| 13| | 6| 14| | 5| 15| | 96| 16| | 14| 17| | 26| 18| | 81| 19| +----+---+ 这里show是默认显示20行数据 | show(100)可指定行数 将df建立为临时表，之后就可以用我们熟悉的sql语句操作了，提醒一下SparkSQL支持的sql是HiveSQL而它又与Oracle语法类似，但与MySQL语法有些差别 df.createOrReplaceTempView(&quot;temp&quot;) 为了方便观看我们将他们的列换下位置，用SQL语句的形式再建一个临时表，这种方式在操作df时很常用 val df2 = sql(&quot;select id,data from temp&quot;) df2.createOrReplaceTempView(&quot;temp2&quot;) sql(&quot;select * from temp2&quot;).show +---+----+ | id|data| +---+----+ | 0| 85| | 1| 54| | 2| 69| | 3| 7| | 4| 39| | 5| 40| 此时我们准备工作完成，开始正式写sql，大致思路为：增加两列，第一列让数列整体上移，第二列让数列整体下移这样就达到了让我们的data列的每一个数字都可以与其相邻的数字作比较的目的，之后用case when end来标识波峰波谷 - 导入隐式转换和sql函数的包 import spark.implicits._ import org.apache.spark.sql.functions sql(&quot;&quot;&quot; select id,data, case when data&gt;data1 and data&gt;data2 then &apos;波峰&apos; when data&lt;data1 and data&lt;data2 then &apos;波谷&apos; end as data3 from( select id,data,lead(data) over(order by id) as data1,lag(data) over(order by id) as data2 from temp2 ) &quot;&quot;&quot;).show - [case when](http://www.cnblogs.com/aipan/p/7770611.html) 用法和if else思路类似 - 在这里我们用了分析函数lead()和lag()，详细用法请[点击这里](https://blog.csdn.net/pelifymeng2/article/details/70313943)，我记这些随缘，忘了就上网查一下，没必要记的太死，关键是一定要有思路 +---+----+-----+ | id|data|data3| +---+----+-----+ | 0| 85| null| | 1| 54| 波谷| | 2| 69| 波峰| | 3| 7| 波谷| | 4| 39| null| | 5| 40| null| | 6| 43| null| | 7| 50| null| | 8| 59| 波峰| | 9| 28| 波谷| | 10| 98| 波峰| | 11| 70| 波谷| | 12| 87| 波峰| | 13| 52| null| | 14| 6| null| | 15| 5| 波谷| | 16| 96| 波峰| | 17| 14| 波谷| | 18| 26| null| | 19| 81| 波峰| +---+----+-----+ 好啦大功告成，有问题可以联系我我们一起交流，其实一直困扰我的问题是不知道为什么用sparkSql时子查询不给查询语句起别名也不报错，而用oracle和mysql时就会报错……616581760 微信QQ同号","categories":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/categories/数据库/"}],"tags":[{"name":"SparkSql","slug":"SparkSql","permalink":"http://yoursite.com/tags/SparkSql/"}]},{"title":"Spark最全笔记","slug":"Spark最全笔记","date":"2019-02-01T18:52:19.000Z","updated":"2019-02-01T19:01:58.007Z","comments":true,"path":"2019/02/02/Spark最全笔记/","link":"","permalink":"http://yoursite.com/2019/02/02/Spark最全笔记/","excerpt":"","text":"Spark简介 三种资源管理器：Yarn、Standalone、Mesos（粗粒度、细粒度）粗粒度管理模式：一次分配运行过程中的全部资源，且运行过程中要一直占用这些资源（即使不用），程序运行结束后，回收全部资源；细粒度管理模式：资源按需分配。 术语解释： application : spark应用程序，包含：driver(一个) + executor(多个)； application jar cluster manager worker node driver program：管理应用程序。 main() + 初始化了SparkContext； executor ： 位于worker，task在这里执行； Deploy mode ： cluster（driver在集群中，看不见输出结果。用于生产环境）、client（driver在客户端，能看见结果。用于开发测试） Task：任务调度的最小单位，Driver将Task发送到Executor上执行；数量由分区数决定 Job：Action触发Job。Job包含多个stage； Stage：依据shuffle（宽依赖），将一个Job切分为多个Stage，Stage又称为TaskSets； Spark部署运行模式：本地模式：Spark所有进程都运行在一台机器的JVM中；local、local[N]、local[*]、local[N,M] 伪分布式模式：在一台机器中模拟集群运行，相关的进程在同一台机器上；local-cluster[N,cores,memory] 分布式模式包括：Standalone、Yarn、Mesosspark://node1:7077 –deploy-mode：cluster、client(缺省值)cluster：看不见返回结果，用于生产环境client：能看见返回结果，用于测试环境二者最主要的区别是：Driver运行在哪里。cluster模式，Driver运行在cluster上；client模式，Driver运行在客户机上；standalone、Yarn都有cluster、client模式； RDDRDD是spark的核心概念，它是一个容错、可以并行执行的分布式数据集。什么是RDD： 一个分区的列表 一个计算函数compute，对每个分区进行计算 对其他RDDs的依赖（宽依赖、窄依赖）列表 对key-value RDDs来说，存在一个分区器（Partitioner）【可选的】 对每个分区有一个优先位置的列表【可选的】 SparkContextSparkContext是编写Spark程序用到的第一个类，是Spark的主要入口点,它负责和整个集群的交互;SparkContext用于连接Spark集群、创建RDD、累加器、广播变量； 调用SparkContext的parallelize、makeRDD方法，从数组中创建RDD；用textFile方法来从文件系统中加载数据创建RDD； SparkContext的三大组件：DAGScheduler、TaskScheduler、SchedulerBackendDAGScheduler : 将DAG划分成若干个Stage；TaskScheduler : 将Stage划分成若干为Task，并对Task进行调度；SchedulerBackend：定义了许多与Executor事件相关的处理。如：新的executor注册时记录executor的信息，增加全局的资源量(核数)；executor更新状态，若任务完成的话，回收core；停止executor、remove executor等事件。 RDD的操作可以分为：Transformation、ActionTransformation只是记录了RDD转换的轨迹，并不会发生真正的计算；只有遇到Action操作时，才会发生真正的计算； 常见的Transformation：会产生Shuffle的Transformation:会产生action的Transformation:常见的Action： PairRDD常见操作：keys、values、sortByKey、reduceByKey、join、mapValues RDD的持久化cache、persist、checkpoint：都属于Transformation，即都是lazy的，需要action触发；cache() = persist(StorageLevel.MEMORY_ONLY)StorageLevel.MEMORY_ONLY：默认的存储级别StorageLevel.MEMORY_AND_DISK_2：本地放一份，远程放一份；persist：可以有更多存储级别的选择；cache、persist：主要用做性能优化； checkpoint：主要用来做容错：将RDD中的数据存储保存到HFDS；斩断依赖； persist中的数据会被自动清除；checkpoint的数据需要手工清除； RDD分区RDD分区的原则：尽可能使得分区的个数，等于集群核心数目；尽可能使同一 RDD 不同分区内的记录的数量一致； 默认的分区数（并发数）(可在 spark-default.conf 配置)：spark.default.parallelism 对于 textFile 方法，默认情况下：每个HDFS的分区文件（默认块大小128M），每个都会创建一个RDD分区；最小2个分区；对于本地文件，默认分区个数等于 min(defaultParallelism, 2)； 分区器Hash分区、Range分区、自定义分区；只有Key-Value类型的RDD才会有分区器，非Key-Value类型的RDD分区器的值是None。分区器决定了： RDD中分区的个数； RDD中每条数据经过Shuffle过程属于哪个分区； reduce的个数； 依赖RDD的依赖分为两种：窄依赖(Narrow Dependencies)与宽依赖(Wide Dependencies，源码中称为Shuffle Dependencies)依赖有2个作用：其一用来解决数据容错；其二用来划分stage。窄依赖：父RDD的Partition 与 子RDDPartition对应的关系为 1:1 或 n:1；宽依赖：父RDD的Partition 与 子RDDPartition对应的关系为 m:n ；宽依赖对应着shuffle操作；宽依赖中子RDD分区通常来自多个父RDD分区。如果子RDD的某个分区丢失，所有的父RDD分区可能都要进行重新计算； ShuffleHash Shuffle =&gt; Hash Shuffle V2 =&gt; Sort Shuffle =&gt; tungsten-sort =&gt; 两个shuffle算法的合并 =&gt; Hash Shuffle去除Hash Shuffle 算法存在的问题：生成海量的小文件（同时打开过多文件 及 低效的随机IO）reduceByKey、grouByKey ：都有shuffle，但reduceByKey在shuffle过程中传输的数据量小；repartition、coalesce ：coalesce没有shuffle；repartition有shuffle；shuffle是划分stage的依据； 共享变量广播变量、累加器；广播变量，由driver广播到Executor上；在Executor中的Task之间进行共享（减少了数据的传输）；共享变量只读；累加器：则支持在所有不同节点之间进行累加计算，只能由driver读取； 作业调度 SparkContext初始化； Driver向Master注册，并申请资源； Master命令Worker启动Executor； Executor启动，并向Driver注册； Driver向Executor发送Task； Executor向Driver汇报任务执行情况； 应用程序执行完毕，Driver向Master注销自己； SparkSQLSparkSessionRDD、Dataset、DataFrame type DataFrame = Dataset[Row]DataFrame = RDD[Row] + schemaDataset = RDD + case class spark.read.option(“”, true).csv(“”)header、inferschema、delimiter(sep) Actionshow(n, false)printSchema TransformationDSL（领域专用语言）与RDD类似：map、flatMap、filter缓存：cache、presist、checkpointselect ：与列有关wheregroupByorderByjoin空值处理时间日期 分析函数分析函数名(参数) over ( partition by xxx order by xxx rows/range between … and …) 起始行 : unbounded preceding终止行 : unbounded following当前行 ：current row前n行 ： n preceding后n行 ： n following 聚组函数：min、max、sum、count、avg排名函数：row_number、dense_rank、rank行函数：lag、lead、first_value、last_value 其他：cube、rollup记录的合并与展开 ： concat_ws、collect_set、collect_list、explode 半连接（left semi join）：左半连接实现了类似in、exists的查询语义，输出符合条件的左表内容;反连接(left anti join)：两表关联，只返回主表的数据，并且只返回主表与子表没关联上的数据，这种连接就叫反连接。反连接一般就是指的 not in 和 not exists; Kafka &amp; Spark Streaming","categories":[],"tags":[{"name":"Spark","slug":"Spark","permalink":"http://yoursite.com/tags/Spark/"}]},{"title":"Scala最全笔记","slug":"Scala最全笔记","date":"2019-01-31T11:33:11.000Z","updated":"2019-02-01T18:27:17.241Z","comments":true,"path":"2019/01/31/Scala最全笔记/","link":"","permalink":"http://yoursite.com/2019/01/31/Scala最全笔记/","excerpt":"","text":"Scala语言的特点： 基于JVM（可以重用类库） 简洁优雅 面向对象 + 函数式编程（FP） 函数式编程的数学基础是 : λ演算函数式编程中，所有的数据都是不可变的，不同的函数之间通过数据流来交换信息，函数作为FP中的一等公民，享有跟数据一样的地位，可以作为参数传递给下一个函数，同时也可以作为返回值。 基础语法Scala基础 程序文件的名称可以不与对象名称完全匹配； 程序从main()方法开始，main 方法应该在object中； $字符是Scala中的保留关键字； 语句在单行上分号可省略； 操作符是方法，没有提供 ++、–操作符； 使用val声明不可变的值，使用var声明可变的变量；鼓励使用val，避免使用var； 需要给出值或者变量的类型，scala可以做自动类型推断。必要时可以指定类型； 拥有和java一样的数据类型，和java的数据类型的内存布局、精度完全一致; 控制结构和函数 if 语句有返回值； 基础类型都继承自AnyVal，应用类型都继承自AnyRef，AnyVal 与 AnyRef 继承自Any； 块语句是一个包含于{}中的语句序列，块中最后一个表达式的值就是块的值； 字符串插值器：s、raw、f； for循环中：to、until；很方便的写多重循环； for (i &lt;-1 to 3; j&lt;-1 to 3[;] if i != j) print((10*i+j) + “ “ ) for循环中的守卫语句（guard 语句）；for推导式（yield）； Range对象：1 to 100 by 2; Range(1, 100, 2); (1 to 100).toArray; 只要函数不是递归的，不需要指定返回类型。Scala可以进行自动类型推断；函数的定义： def funcname(x: Int, y: Int): Int = {x + y } 默认参数、带名参数、变长参数； 懒值。当val被声明为lazy，它的初始化将被推迟，直到首次对它取值（只有val才能定义为lazy） 数组（Array）Array：长度固定、元素可变、可索引、存放相同类型的集合ArryBuffer：长度可以变；要导包；打印方便toArray / toBuffer增加元素：+=、:+、+:减元素：-=增加一个数组: ++= ++ map、foreach、reduce、flatMapmax、min、sorted、reversesum、product、size、length、indicescount、filter(Not)、distinct、take、takeRigth、takeWhiledrop、dropRigth、dropWhilezip、unzip、head、tail、last、initunion、groupBy、contains Map存放键值对的容器（集合）；默认使用不可变的Map，这里的不可变是指值不可变；如果要给Map增加值应声明为var；如果要使用可变的Map，要显示声明；import scala.collection.mutable.MapgetOrElse Tuple可以存放不同类型的元素，元组是不可变的；最大的长度22；访问使用x._1、x._2 面向对象大处使用面向对象；小处使用函数式编程；（操作数据） 用class声明类，用object声明单例对象；用final阻止一个对象被继承；用abstract阻止对象被实例化；用this指代对象本身；缺省的访问限定符为public； 类 &amp; 对象定义类的成员变量访问属性：class Student(var name: String, var age)class Student(val name: String, val age)class Student(private val name: String, val age)【示意】class Student(private[this] val name: String, val age)【示意】自定义getter、setter方法；定义java风格的getter、setter方法； 主构造函数：就是类体本身 辅助构造函数：命名为this，跟不同的参数列表；第一行必须调用主构造器 或 其他辅助构造器； 用来定义常量和工具方法；实现单例类；与java中的static 类与方法类似；没有参数列表；其他的性质与class类似； 程序的入口点，main方法定义在其中apply方法也放在这里； 继承 &amp; 特质scala采用object关键字实现单例对象，具备和Java静态方法同样的功能；object本质上可以拥有类的所有特性，除了不能提供构造器参数；对于任何在Java中用单例对象的地方，在scala中都可以用object实现： 作为存放工具函数或常量的地方 高效地共享单个不可变实例 当单例对象与某个类具有相同的名称时，它被称为这个类的“伴生对象”，类称为伴生类；类和它的伴生对象必须存在于同一个文件中，而且可以相互访问私有成员（字段和方法）；object的apply、unapply、update方法定义伴生对象中的apply方法，主要用来解决简化对象的初始； trait（接口），是scala代码重用的基本单元，可以同时拥有抽象方法和具体方法； 函数式编程case class &amp; case object样例类是scala中特殊的类。当声明样例类时，如下事情会自动发生： 构造器中每一个参数都成为val（不建议使用var）； 提供apply方法； 提供unapply方法； 将生成toString、equals、hashCode和copy方法； 继承了Product和Serializable； case class和其他类型完全一样，可以添加方法和字段，可以继承；case class最大的用处是用于模式匹配； case class是多例的（后面要跟构造参数），case object是单例的 模式匹配模式匹配使用 match… { case … }的语法结构，具有以下特点： match 语句有返回值； case分支语句不会贯穿到下一个case语句； 如果没有任何一个模式匹配上，会抛出异常（MatchError）； 通常最后一个语句与 _ 匹配； 模式匹配无处不在； ## Option类型 ##Option是Scala编程中常用的一个类，用来避免指针异常；包含两个实例：Some、None；Option类型提供了getOrElse方法；可将Option[T]看做是一个集合，这个集合只要么 只包含一个元素（被包装在Some中）； 要么就不存在元素（返回None）； 函数函数字面量：函数的值；(x, str) =&gt; { x + str.length }函数的类型：(Int, String) =&gt; Int匿名函数：即Lambda表达式，没有名称的函数；可使用下划线作为一个或多个参数的占位符，只要每个参数在函数字面量内仅出现一次。多个下划线指代多个参数； 高阶函数：接收一个或多个函数作为输入 或 输出一个函数。闭包是一种特殊函数，是在其上下文中引用了自由变量的函数；（闭包反映了一个从开放到封闭的过程）柯里化：将原来接收两个参数的函数变成新的接收一个参数的函数的过程；部分应用函数（偏应用函数）：是指缺少部分（甚至全部）参数的函数；偏函数：只接受部分输入参数的函数； 在偏函数中只能使用 case 语句，整个函数必须用大括号包围；通常使用偏函数进行类型转换，拆解； 集合Seq（序列）、Set（集）、map（映射）集合的分类：有序、无序；可变、不可变；默认提供不可变版本，使用可变版本需要显示声明； SeqSeq：具有一定长度的可迭代访问的对象，每个元素均带有一个从0开始计数的固定索引位置。常见的Seq类型包括：List、Vector、String、Range、Queue、Stack；List : 列表是不可变的（元素不能通过赋值改变）；结构是递归的；List递归的表示 ：head :: tial常用操作符 ：Nil、::、:::（++）Vector是以树形结构的形式实现，每个节点可以有不超过32个子节点。（4层可存放100W个节点）；Range通常表示一个整数序列； set与数学集合的概念较为类似。元素没有重复值，不保证元素的存放顺序； 高阶函数flatMap、flatten、reducefold、foldLeft、foldRightaggregate 隐式转换隐式转换可以为现有的类库添加功能; 隐式转隐为隐藏了相应的细节;隐式函数是指那种以implicit关键字声明的带有单个参数的函数; 隐式转换规则：1、源 或 目标类型 的伴生对象中的隐式函数2、当前作用域的隐式函数 隐式参数有点类似缺省参数，如果在调用方法时没有提供某个参数，编译器会在当前作用域查找是否有符合条件的 implicit 对象可以作为参数传入； Ordered、OrderingOrdered 提供比较器模板，可以自定义多种比较方式（我们常用它来解决对象的排序问题）Ordering 定义了相同类型间的比较方式【Ordering.Int、Ordering.String.reverse】 sorted、sortWith、sortBy 函数式编程的特点不变性函数式一等公民递归和尾递归 函数式编程相关的技术少用循环和遍历，多使用 map 、 reduce等函数；柯里化（currying）；高阶函数（higher order function）；递归（recursing）和尾递归管道（pipeline） 函数 与 方法使用 def 定义的是方法；使用 val 定义的是函数；二者在大多数情况下可以认为是相等的； 方法是类的组成部分，不能单独存在；函数是一个完整的对象，可以独立存在；方法可以转化为函数； 下划线的用法：导包类成员变量赋值可变参数类型通配符模式匹配Tuple访问简化函数字面量定义setter方法部分应用函数方法转换为函数 +:、:+、++、::、::: type：声明类型，提供可读性","categories":[],"tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"}]},{"title":"SpringCloud搭建","slug":"SpringCloud搭建","date":"2019-01-31T10:40:32.000Z","updated":"2019-04-17T10:54:08.527Z","comments":true,"path":"2019/01/31/SpringCloud搭建/","link":"","permalink":"http://yoursite.com/2019/01/31/SpringCloud搭建/","excerpt":"","text":"SpringCloudSpringBoot+SpringCloud应用，实现微服务架构。 SpringBoot:快速开发SpringMVC服务API SpringCloud:服务管理，例如服务注册、服务查找、服务调用、集群、容错机制、监控、统一配置、消息总线等。 搭建SpringCloud中心SpringBoot 2.0.1和SpringCloud Finchley.RELEASE 在pom.xml添加eureka-server包定义 org.springframework.boot spring-boot-starter-parent 2.0.1.RELEASE &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- eureka-server --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- spring-cloud-parent --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Finchley.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; 在application.properties定义eureka参数 server.port=1111 eureka.client.registerWithEureka=false eureka.client.fetchRegistry=false eureka.client.serviceUrl.defaultZone=http://localhost:1111/eureka 编写启动类，追加启动注解标记 @EnableEurekaServer @SpringBootApplication public class EurekaBootApplication { public static void main(String[] args) { SpringApplication.run(EurekaBootApplication.class, args); } } 启动应用，打开浏览器测试 http://localhost:1111 Eureka服务中心(将服务注册到Eureka中心) 在pom.xml追加eureka-client定义 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;!-- spring-cloud-parent --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Finchley.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; 在application.properties定义eureka参数 spring.application.name=USER-SERVICE eureka.client.serviceUrl.defaultZone=http://localhost:1111/eureka 在启动类前，追加启动标记 @EnableDiscoveryClient//这是启动标记 @SpringBootApplication @MapperScan(basePackages={&quot;cn.yixue.user.dao&quot;}) @ServletComponentScan public class UserBootApplication { public static void main(String[] args) { SpringApplication.run(UserBootApplication.class, args); } } Ribbon负载均衡(查找Eureka服务调用)采用集群部署服务时，需要使用ribbon负载均衡调用机制。SpringCloud提供的ribbon默认机制为轮询模式。此外还支持随机、权重等模式。 ribbon可以和RestTemplate结合，也可以和feign接口模式结合。 采用RestTemplate模式 在pom.xml添加netflix-ribbon负载定义 org.springframework.cloud spring-cloud-starter-netflix-ribbon 在SpringBootApplication中创建RestTemplate对象时追加@LoadBalaced @Bean @LoadBalanced//启用ribbon负载 public RestTemplate restTemplate(){ return new RestTemplate(); } 使用RestTemplate调用远程服务 String url = “http://YIXUE-USER/user/ticket&quot;; MultiValueMap&lt;String, Object&gt; params = new LinkedMultiValueMap&lt;&gt;(); params.set(“userId”, userId); params.set(“ticket”, ticket); OvlsResult result = restTemplate.postForObject( url, params, ReturnResult.class); 采用Feign远程接口模式(Fegin接口调用) ribbon完全内置，jar包内部依赖，自动作用接口。 在pom.xml添加openfeign定义 org.springframework.cloud spring-cloud-starter-netflix-ribbon org.springframework.cloud spring-cloud-starter-openfeign 编写远程服务的Remote接口(定义Fegin接口) @FeignClient(name=”YIXUE-USER”) public interface UserRemote { @PostMapping(&quot;/user/ticket&quot;) public OvlsResult checkTicket( @RequestParam(name=&quot;userId&quot;,required=false)String userId, @RequestParam(name=&quot;ticket&quot;,required=false)String token); } 注意：参数多余一个，一定要使用@RequestParam标记 注入Remote接口对象使用(注入Feigin接口对象，实现远程服务调用) @Autowired private UserRemote userRemote; //Feign接口调用模式 ReturnResult result = userRemote.ticket(Integer.parseInt(userId), ticket); 在启动类前添加@EnableFeignClients标记 @EnableFeignClients//启用feign @EnableDiscoveryClient//Eureka服务中心启动标记 @SpringBootApplication @MapperScan(basePackages={“cn.yixue.video.dao”}) @ServletComponentScan public class VideoBootApplication { public static void main(String[] args) { SpringApplication.run(VideoBootApplication.class, args); } } Hystrix断路器SpringCloud提供了一个hystrix套件，俗称断路器，可以在服务间调用，被调用方处理缓慢或瘫痪，对调用者提供保护机制。 在pom.xml中添加hystrix工具包 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt; &lt;/dependency&gt; 在启动类前面加@EnableCircuitBreaker @EnableCircuitBreaker//启用断路器hystrix 在Controller方法前使用@HystrixCommand指定延迟或异常后调用的方法 @RestController public class VideoController { @Autowired private VideoService videoService; @HystrixCommand(fallbackMethod=&quot;defaultLoad&quot;) @GetMapping(&quot;/video/{id}&quot;) public OvlsResult load(@PathVariable(&quot;id&quot;)int id){ return videoService.load(id); } public OvlsResult defaultLoad(int id){ ReturnResult result = new OvlsResult(); result.setStatus(OvlsConstant.ERROR1); result.setMsg(OvlsConstant.SELECT_EMPTY_MSG); return result; } } 提示：默认超时设置为1秒，超过一秒就调用fallbackMethod处理。","categories":[],"tags":[{"name":"springCloud","slug":"springCloud","permalink":"http://yoursite.com/tags/springCloud/"}]},{"title":"Hello World","slug":"hello-world","date":"2019-01-30T14:29:47.047Z","updated":"2019-01-30T14:29:47.047Z","comments":true,"path":"2019/01/30/hello-world/","link":"","permalink":"http://yoursite.com/2019/01/30/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}